{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0c14607c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: ANOVA and Eta-Squared (η²)\n",
      "Target: Rank (int64)\n",
      "Column: Type (object)\n",
      "\n",
      "f: 2874.962\n",
      "cv: 3.842\n",
      "p: 0.000\n",
      "Eta-Squared: 0.137\n",
      "\n",
      "Correlation: Medium\n",
      "\n",
      "---------------------------------------------------------\n",
      "\n",
      "Test: ANOVA and Eta-Squared (η²)\n",
      "Target: Rank (int64)\n",
      "Column: Genre (object)\n",
      "\n",
      "f: 44.435\n",
      "cv: 1.486\n",
      "p: 0.000\n",
      "Eta-Squared: 0.063\n",
      "\n",
      "Correlation: Medium\n",
      "\n",
      "---------------------------------------------------------\n",
      "\n",
      "Test: Spearmans Correlation Coefficient\n",
      "Target: Rank (int64)\n",
      "Column: Watchtime in Million (float64)\n",
      "\n",
      "x: -0.995\n",
      "p: 0.000\n",
      "\n",
      "Correlation: Large Negative\n",
      "\n",
      "---------------------------------------------------------\n",
      "\n",
      "Correlation Between Target and Columns\n",
      "\n",
      "Target: Rank\n",
      "\n",
      "Large\n",
      " - Watchtime in Million\n",
      "Medium\n",
      " - Type\n",
      " - Genre\n",
      "None\n",
      " - Premiere\n",
      "Unable\n",
      " - Title\n"
     ]
    }
   ],
   "source": [
    "# Program Purpose: Find Correlation between target column and all other columns. Automates statistical \n",
    "# tests of correlation as well as allows any analyst to find association between all types of variables \n",
    "# without extensive statistical experience.\n",
    "\n",
    "# First, the program takes an input of a excel file or a csv file and\n",
    "# then have the user input a target column. This target will then be compared to all of the \n",
    "# other columns through the use of statistical tests. These tests will then determine correlation\n",
    "# between the target and the columns. The program classifies each of these correlations as\n",
    "# either large, medium, small, or none. The program outputs a list of all of these correlations\n",
    "# based on the category that they were assigned. \n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "#df = pd.read_excel('C:/Users/Ted/Downloads/FinancialSample.xlsx')\n",
    "df = pd.read_csv('C:/Users/Ted/Downloads/flixpatrol.csv')\n",
    "\n",
    "target = 'Rank'                          # Segment and Units Sold are good test samples\n",
    "bins = int(np.sqrt(len(df[target])))         # Default: int(np.sqrt(len(df[target])))\n",
    "max_unique_values_threshold = 50             # Default: 20. This is the number of categories that an object is limited to having, chi2 gets messed up when there are too many\n",
    "showCrosstab = False\n",
    "showBinnedCrosstab = False         # Depending on the dataset, these crosstabs can be very large and hard to understand but can still glean information from it if interested\n",
    "showGroupMeans = False          # Not working for now due to the possibility of the order of object and number to be swapped in ANOVA\n",
    "includeExplanations = False        # Not implemented, explanations have been commented out\n",
    "\n",
    "# Here the settings section ends, and the program begins \n",
    "\n",
    "correlation_categories = {\n",
    "    \"large\": [],\n",
    "    \"medium\": [],\n",
    "    \"small\": [],\n",
    "    \"none\": [],\n",
    "    \"unable\": []\n",
    "}\n",
    "\n",
    "# for every column that is not the target column, check the dtype of that \n",
    "# column and the target column and see if they are both object\n",
    "for column in df.columns:\n",
    "    if 'Unnamed' not in column and target != column:\n",
    "        \n",
    "        # this is used to determine if the object (categorical) target or column have too many unique values\n",
    "        # if this is the case, the tests will be unaccurate and should not be used, therefore they are put in the unable column\n",
    "        if (df[target].dtype == 'object' and df[target].nunique() > max_unique_values_threshold):\n",
    "            print(f\"Target '{target}' has {df[column].nunique()} unique values, which is more than the threshold of {max_unique_values_threshold}.\")\n",
    "            print(\"Attempts at correlation would yield unusable results\")\n",
    "            print()\n",
    "            #correlation_categories[\"unable\"].append(column)\n",
    "            break \n",
    "        elif (df[column].dtype == 'object' and df[column].nunique() > max_unique_values_threshold):\n",
    "            #print(f\"Column '{column}' has {df[column].nunique()} unique values, which is more than the threshold.\")\n",
    "            correlation_categories[\"unable\"].append(column)\n",
    "            continue\n",
    "            \n",
    "        \n",
    "        \n",
    "        # object vs object: chi2 and Cramer's V (bias corrected)\n",
    "        # this test compares observed and expected frequencies of the crosstab\n",
    "        if df[target].dtype == 'object' and df[column].dtype == 'object': \n",
    "            \n",
    "            ct = pd.crosstab(df[target], df[column])\n",
    "                       \n",
    "            chi2, p, dof, expected = stats.chi2_contingency(ct)\n",
    "            critical_value = stats.chi2.ppf(1 - 0.05, dof)\n",
    "            \n",
    "            r, k = ct.shape                                      # Number of rows and columns\n",
    "            n = np.sum(ct.values) \n",
    "            k_corrected_term = ((k - 1)**2) / (n - 1)            # Bias correction terms\n",
    "            r_corrected_term = ((r - 1)**2) / (n - 1)\n",
    "            k_corrected = k - k_corrected_term                   # Corrected number of rows and columns for bias correction\n",
    "            r_corrected = r - r_corrected_term\n",
    "                    \n",
    "            if p < 0.05:\n",
    "                if chi2 > critical_value:\n",
    "                    \n",
    "                    print('Test: Chi-Square (Χ²) and Cramers V')\n",
    "                    \n",
    "                    print(f'Target: {target} ({df[target].dtype})')\n",
    "                    print(f'Column: {column} ({df[column].dtype})')\n",
    "                    print()\n",
    "                    \n",
    "                    cramers_v = np.sqrt((chi2 / n) / min(k_corrected - 1, r_corrected - 1))\n",
    "    \n",
    "                    if cramers_v > 0.5:\n",
    "                        doc = 'Large'\n",
    "                        correlation_categories[\"large\"].append(column)\n",
    "                    elif cramers_v > 0.3:\n",
    "                        doc = 'Medium'\n",
    "                        correlation_categories[\"medium\"].append(column)\n",
    "                    elif cramers_v > 0.1:\n",
    "                        doc = 'Small'\n",
    "                        correlation_categories[\"small\"].append(column)\n",
    "                    else:\n",
    "                        doc = 'None'\n",
    "                        correlation_categories[\"none\"].append(column)\n",
    "\n",
    "                    print(f'chi2: {chi2:.3f}')\n",
    "                    print(f'cv: {critical_value:.3f}')          # this is the value that chi squared must be below or above to determine if it is high or low\n",
    "                    print(f'p: {p:.3f}')\n",
    "                    print(f'Cramers V: {cramers_v:.3f}')        # note that Cramers V works but is not suited towards 2x2 contingency tables\n",
    "                    \n",
    "                    print()    \n",
    "                    print('Correlation:', doc)\n",
    "                    \n",
    "                    if showCrosstab == True:\n",
    "                        print()\n",
    "                        print('Crosstab:')\n",
    "                        print()\n",
    "                        print(ct)\n",
    "                        print()\n",
    "                    \n",
    "                    \n",
    "                    #print('p value is below the significance threshold of 0.05')\n",
    "                    #print('chi2 value is greater than critical value')\n",
    "                    #print('the difference between observed and expected frequencies is too large to be attributed to chance')\n",
    "\n",
    "                    print()\n",
    "                    print('---------------------------------------------------------')\n",
    "                    print()\n",
    "                    \n",
    "                else: \n",
    "                    correlation_categories[\"none\"].append(column)\n",
    "                    #print('No Correlation')\n",
    "                    #print()\n",
    "                    #print('p value is above the significance threshold of 0.05')\n",
    "                    #print('chi2 value is less than critical value')\n",
    "                    #print('the difference between observed and expected frequencies is too small and could be attributed to chance')\n",
    "                    \n",
    "            else:\n",
    "                correlation_categories[\"none\"].append(column)\n",
    "                #print('No Correlation')\n",
    "                #print()\n",
    "                #print('p value is above the significance threshold of 0.05')\n",
    "       \n",
    "\n",
    "    \n",
    "\n",
    "        #numerical vs numerical: pearsons or spearmans\n",
    "        elif (df[target].dtype == 'float64' or df[target].dtype == 'int64') and (df[column].dtype == 'float64' or df[column].dtype == 'int64'):\n",
    "            \n",
    "            x, p = stats.spearmanr(df[target], df[column])       # can use either pearsonr or spearmanr depending on how the data is distributed             \n",
    "            \n",
    "            if p < 0.05:\n",
    "                \n",
    "                print('Test: Spearmans Correlation Coefficient')\n",
    "                \n",
    "                if x >= 0.5:\n",
    "                    doc = 'Large Positive'\n",
    "                    correlation_categories[\"large\"].append(column)\n",
    "                elif x >= 0.3:\n",
    "                    doc = 'Medium Positive'\n",
    "                    correlation_categories[\"medium\"].append(column)\n",
    "                elif x >= 0.1:\n",
    "                    doc = 'Small Positive'\n",
    "                    correlation_categories[\"small\"].append(column)\n",
    "                elif x < 0.1 and x > -0.1:\n",
    "                    doc = 'None'\n",
    "                    correlation_categories[\"none\"].append(column)\n",
    "                elif x <= -0.1 and x > -0.3:\n",
    "                    doc = 'Small Negative'\n",
    "                    correlation_categories[\"small\"].append(column)\n",
    "                elif x <= -0.3 and x > -0.5:\n",
    "                    doc = 'Medium Negative'\n",
    "                    correlation_categories[\"medium\"].append(column)\n",
    "                elif x <= -0.5 and x >= -1:\n",
    "                    doc = 'Large Negative'\n",
    "                    correlation_categories[\"large\"].append(column)\n",
    "\n",
    "                print(f'Target: {target} ({df[target].dtype})')\n",
    "                print(f'Column: {column} ({df[column].dtype})')\n",
    "                print()\n",
    "\n",
    "                print(f'x: {x:.3f}')\n",
    "                print(f'p: {p:.3f}')\n",
    "\n",
    "                print()\n",
    "                \n",
    "                print('Correlation:', doc)\n",
    "                #print('p value is below the significance threshold of 0.05')    \n",
    "                \n",
    "                print()\n",
    "                print('---------------------------------------------------------')\n",
    "                print()\n",
    "                \n",
    "            else:\n",
    "                correlation_categories[\"none\"].append(column)\n",
    "                #print('No Correlation')\n",
    "                #print('Reason: p value is above the significance threshold of 0.05')\n",
    "              \n",
    "\n",
    "            \n",
    "            \n",
    "        #numerical vs object: ANOVA\n",
    "        #There can be no technical 'correlation' between numerical vs categorical, only the result is something very similar \n",
    "        elif ((df[target].dtype == 'float64' or df[target].dtype == 'int64') and df[column].dtype == 'object'):    \n",
    "            \n",
    "            grouped_data = df.groupby(column)[target].apply(list)\n",
    "\n",
    "            # Convert the grouped data into a list of arrays suitable for ANOVA\n",
    "            data_arrays = [group for group in grouped_data]\n",
    "\n",
    "            #print(*data_arrays)\n",
    "            #print(grouped_data)\n",
    "            #print(grouped_data)\n",
    "                \n",
    "            # Perform ANOVA\n",
    "            f, p = stats.f_oneway(*data_arrays)\n",
    "            n = len(df.index)        # number of rows\n",
    "            # Finding degrees of freedom for both numerator and demoninator, then using those to help find critical value\n",
    "            dfn, dfd = (len(grouped_data) - 1), (n - 2)      # (2-1) because we are only comparing 2 groups, len(df.index) is number of rows\n",
    "            critical_value = stats.f.ppf(1 - 0.05, dfn, dfd)\n",
    "            \n",
    "            \n",
    "            if p < 0.05:\n",
    "                if f > critical_value:\n",
    "                    \n",
    "                    print('Test: ANOVA and Eta-Squared (η²)')\n",
    "                    \n",
    "                    # Calculating the 3 values here that are needed to find the eta^2\n",
    "                    # Calculate Total Sum of Squares (TSS)  \n",
    "                    # all_data = np.concatenate(grouped_data)\n",
    "                    all_data = []\n",
    "                    for group in grouped_data:\n",
    "                        all_data.extend(group)\n",
    "                    all_data = np.array(all_data)\n",
    "                    grand_mean = np.mean(all_data)\n",
    "                    tss = np.sum((all_data - grand_mean) ** 2)\n",
    "\n",
    "                    # Calculate Within-Group Sum of Squares (WSS)\n",
    "                    wss = sum(sum((group - np.mean(group)) ** 2) for group in grouped_data)\n",
    "\n",
    "                    # Calculate Between-Group Sum of Squares (BSS)\n",
    "                    bss = tss - wss\n",
    "\n",
    "                    # Calculate Eta-squared (η²)\n",
    "                    eta_squared = bss / tss\n",
    "                    \n",
    "                    if eta_squared > 0.14:\n",
    "                        doc = 'Large'\n",
    "                        correlation_categories[\"large\"].append(column)\n",
    "                    elif eta_squared > 0.06:\n",
    "                        doc = 'Medium'\n",
    "                        correlation_categories[\"medium\"].append(column)\n",
    "                    elif eta_squared > 0.01:\n",
    "                        doc = 'Small'\n",
    "                        correlation_categories[\"small\"].append(column)\n",
    "                    else:\n",
    "                        doc = 'None'\n",
    "                        correlation_categories[\"none\"].append(column)\n",
    "                        \n",
    "                    print(f'Target: {target} ({df[target].dtype})')\n",
    "                    print(f'Column: {column} ({df[column].dtype})')\n",
    "                    \n",
    "                    if showGroupMeans == True:\n",
    "                        print()\n",
    "                        print('Table of Means by Each Group')\n",
    "                        print()\n",
    "                        group_means = df.groupby(column)[target].mean().reset_index()\n",
    "                        print(round(group_means, 2))\n",
    "                    \n",
    "                    print()\n",
    "                    print(f\"f: {f:.3f}\")\n",
    "                    print(f\"cv: {critical_value:.3f}\", )\n",
    "                    print(f\"p: {p:.3f}\")\n",
    "                    print(f\"Eta-Squared: {eta_squared:.3f}\")     \n",
    "                    print()\n",
    "                    \n",
    "                    print('Correlation:', doc)\n",
    "                    \n",
    "                    print()\n",
    "                    #print('p value is below the significance threshold of 0.05')\n",
    "                    #print('f value is greater than critical value')\n",
    "                    #print('the difference between observed and expected frequencies is too large to be attributed to chance')\n",
    "\n",
    "                    print('---------------------------------------------------------')\n",
    "                    print()    \n",
    "                    \n",
    "                else:\n",
    "                    correlation_categories[\"none\"].append(column)\n",
    "                    #print('No Correlation')\n",
    "                    #print()\n",
    "                    #print('p value is above the significance threshold of 0.05')\n",
    "                    #print('f value is less than critical value')\n",
    "                    #print('the difference between observed and expected frequencies is too small and could be attributed to chance')\n",
    "                    \n",
    "            else:\n",
    "                correlation_categories[\"none\"].append(column)\n",
    "                #print('No Correlation')\n",
    "                #print()\n",
    "                #print('p value is above the significance threshold of 0.05')\n",
    "                \n",
    "                \n",
    "                \n",
    "        # using chi2 test, however I use bins that separate the numerical variable into a calculated number of bins\n",
    "        # finding correlation between dependent categorical variable and independent continuous variable is quite\n",
    "        # hard, and using chi2 with bins has been the most reasonable approach that I have been able to find\n",
    "        elif (df[target].dtype == 'object' and (df[column].dtype == 'float64' or df[column].dtype == 'int64')):\n",
    "            \n",
    "            bin_edges = np.linspace(1, df[column].max(), num=bins+1)\n",
    "            ct = pd.crosstab(df[target], pd.cut(df[column], bins=bin_edges, right=False, include_lowest=True), margins=False)\n",
    "            #print(pd.cut(df[column], bin_edges).unique())\n",
    "            \n",
    "            chi2, p, dof, expected = stats.chi2_contingency(ct)\n",
    "            critical_value = stats.chi2.ppf(1 - 0.05, dof)\n",
    "            \n",
    "            r, k = ct.shape                                      # Number of rows and columns\n",
    "            n = np.sum(ct.values) \n",
    "            k_corrected_term = ((k - 1)**2) / (n - 1)            # Bias correction terms\n",
    "            r_corrected_term = ((r - 1)**2) / (n - 1)\n",
    "            k_corrected = k - k_corrected_term                   # Corrected number of rows and columns for bias correction\n",
    "            r_corrected = r - r_corrected_term\n",
    "                    \n",
    "            if p < 0.05:\n",
    "                if chi2 > critical_value:\n",
    "                    \n",
    "                    print('Test: Binned Chi-Square (Χ²) and Cramers V')\n",
    "                    \n",
    "                    print(f'Target: {target} ({df[target].dtype})')\n",
    "                    print(f'Column: {column} ({df[column].dtype})')\n",
    "                    print()\n",
    "                    \n",
    "                    cramers_v = np.sqrt((chi2 / n) / min(k_corrected - 1, r_corrected - 1))\n",
    "    \n",
    "                    if cramers_v > 0.5:\n",
    "                        doc = 'Large'\n",
    "                        correlation_categories[\"large\"].append(column)\n",
    "                    elif cramers_v > 0.3:\n",
    "                        doc = 'Medium'\n",
    "                        correlation_categories[\"medium\"].append(column)\n",
    "                    elif cramers_v > 0.1:\n",
    "                        doc = 'Small'\n",
    "                        correlation_categories[\"small\"].append(column)\n",
    "                    else:\n",
    "                        doc = 'None'\n",
    "                        correlation_categories[\"none\"].append(column)\n",
    "\n",
    "                    print(f'chi2: {chi2:.3f}')\n",
    "                    print(f'cv: {critical_value:.3f}')          # this is the value that chi squared must be below or above to determine if it is high or low\n",
    "                    print(f'p: {p:.3f}')\n",
    "                    print(f'Cramers V: {cramers_v:.3f}')        # note that Cramers V works but is not suited towards 2x2 contingency tables\n",
    "                    \n",
    "                    print()    \n",
    "                    print('Correlation:', doc)\n",
    "\n",
    "                    print()\n",
    "                    print('---------------------------------------------------------')\n",
    "                    print()\n",
    "                    \n",
    "                    if showBinnedCrosstab == True:\n",
    "                        print('Binned Crosstab:')\n",
    "                        print()\n",
    "                        print(ct)\n",
    "                        print()\n",
    "                    \n",
    "                else: \n",
    "                    correlation_categories[\"none\"].append(column)\n",
    "\n",
    "            else:\n",
    "                correlation_categories[\"none\"].append(column)\n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "print('Correlation Between Target and Columns')\n",
    "print()\n",
    "print('Target:', target)\n",
    "print()\n",
    "    \n",
    "# printing out the values in each category\n",
    "for category, values in correlation_categories.items():\n",
    "    if values:\n",
    "        print(category.capitalize())\n",
    "        for value in values:\n",
    "            print(f\" - {value}\")\n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "12e456fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "int64 - Rank\n",
      "object - Title\n",
      "object - Type\n",
      "float64 - Premiere\n",
      "object - Genre\n",
      "float64 - Watchtime in Million\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "for column in df.columns:\n",
    "    print(f'{df[column].dtype} - {column}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cda7f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
