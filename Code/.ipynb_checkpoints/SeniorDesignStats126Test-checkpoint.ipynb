{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89af2ed3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: Units Sold\n",
      "Column: Country\n",
      "\n",
      "f: 4.446188124281453\n",
      "Critical Value: 2.384692685857469\n",
      "p: 0.0014863609272251354\n",
      "\n",
      "Eta-squared: 0.024951084369888396\n",
      "\n",
      "Correlation: Small\n",
      "\n",
      "---------------------------------------------------------\n",
      "\n",
      "Target: Units Sold\n",
      "Column: Gross Sales\n",
      "\n",
      "x: 0.32722066254045745\n",
      "p: 6.232960229375536e-19\n",
      "\n",
      "Correlation: Medium Positive\n",
      "\n",
      "---------------------------------------------------------\n",
      "\n",
      "Target: Units Sold\n",
      "Column: Discounts\n",
      "\n",
      "x: 0.2530479774656188\n",
      "p: 1.0911815841023358e-11\n",
      "\n",
      "Correlation: Small Positive\n",
      "\n",
      "---------------------------------------------------------\n",
      "\n",
      "Target: Units Sold\n",
      "Column: Sales\n",
      "\n",
      "x: 0.3269139725274527\n",
      "p: 6.747469128497393e-19\n",
      "\n",
      "Correlation: Medium Positive\n",
      "\n",
      "---------------------------------------------------------\n",
      "\n",
      "Target: Units Sold\n",
      "Column: COGS\n",
      "\n",
      "x: 0.33169439250128174\n",
      "p: 1.9397416204430017e-19\n",
      "\n",
      "Correlation: Medium Positive\n",
      "\n",
      "---------------------------------------------------------\n",
      "\n",
      "Target: Units Sold\n",
      "Column: Profit\n",
      "\n",
      "x: 0.2284369068302451\n",
      "p: 9.7107456560683e-10\n",
      "\n",
      "Correlation: Small Positive\n",
      "\n",
      "---------------------------------------------------------\n",
      "\n",
      "Target: Units Sold\n",
      "Column: Month Number\n",
      "\n",
      "x: -0.10360689389535777\n",
      "p: 0.006075681541677351\n",
      "\n",
      "Correlation: Small Negative\n",
      "\n",
      "---------------------------------------------------------\n",
      "\n",
      "Target: Units Sold\n",
      "Column: Month Name\n",
      "\n",
      "f: 4.208747940899694\n",
      "Critical Value: 1.8023578677863998\n",
      "p: 4.666935538727093e-06\n",
      "\n",
      "Eta-squared: 0.06304843416802154\n",
      "\n",
      "Correlation: Medium\n",
      "\n",
      "---------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "df = pd.read_excel('C:/Users/Ted/Downloads/FinancialSample.xlsx')\n",
    "#df = pd.read_csv('C:/Users/Ted/Downloads/FinancialSample.csv')\n",
    "\n",
    "target = 'Units Sold'         #Segment and Units Sold are good test samples\n",
    "showCrosstab = False\n",
    "showGroupMeans = False        # Not working for now due to the possibility of the order of object and number to be swapped in ANOVA\n",
    "includeExplanations = True    # Not implemented\n",
    "\n",
    "correlation_categories = {\n",
    "    \"small\": [],\n",
    "    \"medium\": [],\n",
    "    \"large\": []\n",
    "}\n",
    "\n",
    "# for every column that is not the target column, check the dtype of that \n",
    "# column and the target column and see if they are both object\n",
    "for column in df.columns:\n",
    "    if target != column:\n",
    "        #object vs object: chi2\n",
    "        # this test compares observed and expected frequencies of the crosstab\n",
    "        if df[target].dtype == 'object' and df[column].dtype == 'object':\n",
    "            ct = pd.crosstab(df[target], df[column])\n",
    "            if showCrosstab == True:\n",
    "                print(ct)\n",
    "                print()\n",
    "            \n",
    "            chi2, p, dof, expected = stats.chi2_contingency(ct)\n",
    "            critical_value = stats.chi2.ppf(1 - 0.05, dof)\n",
    "            \n",
    "                    \n",
    "            if p < 0.05:\n",
    "                if chi2 > critical_value:\n",
    "                    \n",
    "                    print(f'Target: {target}')\n",
    "                    print(f'Column: {column}')\n",
    "                    print()\n",
    "                    \n",
    "                    n = np.sum(ct.values) \n",
    "                    cramers_v = np.sqrt(chi2 / (n * (min(ct.shape) - 1)))\n",
    "    \n",
    "                    if cramers_v > 0.5:\n",
    "                        doc = 'Large'\n",
    "                \n",
    "                    elif cramers_v > 0.3:\n",
    "                        doc = 'Medium'\n",
    "                    elif cramers_v > 0.1:\n",
    "                        doc = 'Small'\n",
    "                    else:\n",
    "                        doc = 'None'\n",
    "\n",
    "                    print(f'chi2: {chi2}')\n",
    "                    print(f'Critical Value: {critical_value}')          # this is the value that chi squared must be below or above to determine if it is high or low\n",
    "                    print(f'p: {p}')\n",
    "                    \n",
    "                    print()    \n",
    "                    print('Correlation:', doc)\n",
    "                    print(f'Cramers V: {cramers_v}')            # note that Cramers V works but is not suited towards 2x2 contingency tables\n",
    "            \n",
    "                    #print('p value is below the significance threshold of 0.05')\n",
    "                    #print('chi2 value is greater than critical value')\n",
    "                    #print('the difference between observed and expected frequencies is too large to be attributed to chance')\n",
    "\n",
    "                    print()\n",
    "                    print('---------------------------------------------------------')\n",
    "                    print()\n",
    "                    \n",
    "                else: \n",
    "                    pass\n",
    "                    #print('No Correlation')\n",
    "                    #print()\n",
    "                    #print('p value is above the significance threshold of 0.05')\n",
    "                    #print('chi2 value is less than critical value')\n",
    "                    #print('the difference between observed and expected frequencies is too small and could be attributed to chance')\n",
    "                    \n",
    "            else:\n",
    "                pass\n",
    "                #print('No Correlation')\n",
    "                #print()\n",
    "                #print('p value is above the significance threshold of 0.05')\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "            \n",
    "        #numerical vs numerical: pearsons or spearmans\n",
    "        if (df[target].dtype == 'float64' or df[target].dtype == 'int64') and (df[column].dtype == 'float64' or df[column].dtype == 'int64'):\n",
    "            \n",
    "            x, p = stats.pearsonr(df[target], df[column])       # can use either pearsonr or spearmanr depending on how the data is distributed\n",
    "                         \n",
    "            \n",
    "            if p < 0.05:\n",
    "                \n",
    "                if x >= 0.5:\n",
    "                    doc = 'Large Positive'\n",
    "                    correlation_categories[\"large\"].append(column)\n",
    "                elif x >= 0.3:\n",
    "                    doc = 'Medium Positive'\n",
    "                    correlation_categories[\"medium\"].append(column)\n",
    "                elif x >= 0.1:\n",
    "                    doc = 'Small Positive'\n",
    "                    correlation_categories[\"small\"].append(column)\n",
    "                elif x < 0.1 and x > -0.1:\n",
    "                    doc = 'None'\n",
    "                elif x <= -0.1 and x > -0.3:\n",
    "                    doc = 'Small Negative'\n",
    "                    correlation_categories[\"small\"].append(column)\n",
    "                elif x <= -0.3 and x > -0.5:\n",
    "                    doc = 'Medium Negative'\n",
    "                    correlation_categories[\"medium\"].append(column)\n",
    "                elif x <= -0.5 and x >= -1:\n",
    "                    doc = 'Large Negative'\n",
    "                    correlation_categories[\"large\"].append(column)\n",
    "                \n",
    "                print(f'Target: {target}')\n",
    "                print(f'Column: {column}')\n",
    "                print()\n",
    "\n",
    "                print(f'x: {x}')\n",
    "                print(f'p: {p}')\n",
    "\n",
    "                print()\n",
    "                \n",
    "                print('Correlation:', doc)\n",
    "                #print('p value is below the significance threshold of 0.05')    \n",
    "                \n",
    "                print()\n",
    "                print('---------------------------------------------------------')\n",
    "                print()\n",
    "                \n",
    "            else:\n",
    "                pass\n",
    "                #print('No Correlation')\n",
    "                #print('Reason: p value is above the significance threshold of 0.05')\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        #object vs numerical: ANOVA\n",
    "        #There can be no correlation between nominal vs numerical, only something similar \n",
    "        if (df[target].dtype == 'object' and (df[column].dtype == 'float64' or df[column].dtype == 'int64')) or ((df[target].dtype == 'float64' or df[target].dtype == 'int64') and df[column].dtype == 'object'):    \n",
    "            \n",
    "            if df[target].dtype == 'object':\n",
    "                grouped_data = df.groupby(target)[column].apply(list)\n",
    "            else:\n",
    "                grouped_data = df.groupby(column)[target].apply(list)\n",
    "\n",
    "            # Convert the grouped data into a list of arrays suitable for ANOVA\n",
    "            data_arrays = [group for group in grouped_data]\n",
    "\n",
    "            #print(*data_arrays)\n",
    "            #print(grouped_data)\n",
    "            #print(grouped_data)\n",
    "                \n",
    "            # Perform ANOVA\n",
    "            f, p = stats.f_oneway(*data_arrays)\n",
    "            n = len(df.index)        # number of rows\n",
    "            # Finding degrees of freedom for both numerator and demoninator, then using those to help find critical value\n",
    "            dfn, dfd = (len(grouped_data) - 1), (n - 2)      # (2-1) because we are only comparing 2 groups, len(df.index) is number of rows\n",
    "            critical_value = stats.f.ppf(1 - 0.05, dfn, dfd)\n",
    "    \n",
    "            \n",
    "            # Calculating the 3 values here that are needed to find the eta^2\n",
    "            # Calculate Total Sum of Squares (TSS)  \n",
    "            # all_data = np.concatenate(grouped_data)\n",
    "            all_data = []\n",
    "            for group in grouped_data:\n",
    "                all_data.extend(group)\n",
    "            all_data = np.array(all_data)\n",
    "            grand_mean = np.mean(all_data)\n",
    "            tss = np.sum((all_data - grand_mean) ** 2)\n",
    "\n",
    "            # Calculate Within-Group Sum of Squares (WSS)\n",
    "            wss = sum(sum((group - np.mean(group)) ** 2) for group in grouped_data)\n",
    "\n",
    "            # Calculate Between-Group Sum of Squares (BSS)\n",
    "            bss = tss - wss\n",
    "\n",
    "            # Calculate Eta-squared (η²)\n",
    "            eta_squared = bss / tss\n",
    "            \n",
    "            \n",
    "            if p < 0.05:\n",
    "                if f > critical_value:\n",
    "                    \n",
    "                    if eta_squared > 0.14:\n",
    "                        doc = 'Large'\n",
    "                        correlation_categories[\"large\"].append(column)\n",
    "                    elif eta_squared > 0.06:\n",
    "                        doc = 'Medium'\n",
    "                        correlation_categories[\"medium\"].append(column)\n",
    "                    elif eta_squared > 0.01:\n",
    "                        doc = 'Small'\n",
    "                        correlation_categories[\"small\"].append(column)\n",
    "                    else:\n",
    "                        doc = 'None'\n",
    "                        \n",
    "                    print(f'Target: {target}')\n",
    "                    print(f'Column: {column}')  \n",
    "                    \n",
    "                    if showGroupMeans == True:\n",
    "                        print()\n",
    "                        print('Table of Means by Each Group')\n",
    "                        print()\n",
    "                        group_means = df.groupby(column)[target].mean().reset_index()\n",
    "                        print(round(group_means, 2))\n",
    "                    \n",
    "                    print()\n",
    "                    print(\"f:\", f)\n",
    "                    print(f\"Critical Value: {critical_value}\", )\n",
    "                    print(\"p:\", p)\n",
    "                    print()\n",
    "                    \n",
    "                    print(\"Eta-squared:\", eta_squared)     \n",
    "                    print()\n",
    "                    \n",
    "                    print('Correlation:', doc)\n",
    "                    \n",
    "                    print()\n",
    "                    #print('p value is below the significance threshold of 0.05')\n",
    "                    #print('f value is greater than critical value')\n",
    "                    #print('the difference between observed and expected frequencies is too large to be attributed to chance')\n",
    "\n",
    "                    print('---------------------------------------------------------')\n",
    "                    print()    \n",
    "                    \n",
    "                else:\n",
    "                    pass\n",
    "                    #print('No Correlation')\n",
    "                    #print()\n",
    "                    #print('p value is above the significance threshold of 0.05')\n",
    "                    #print('f value is less than critical value')\n",
    "                    #print('the difference between observed and expected frequencies is too small and could be attributed to chance')\n",
    "                    \n",
    "            else:\n",
    "                pass\n",
    "                #print('No Correlation')\n",
    "                #print()\n",
    "                #print('p value is above the significance threshold of 0.05')\n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35524d16",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: Segment\n",
      "Column: Country\n",
      "\n",
      "chi2: 0.0\n",
      "Critical Value: 26.29622760486423\n",
      "p: 1.0\n",
      "\n",
      "Conclusion:\n",
      "No Correlation\n",
      "\n",
      "p value is above the significance threshold of 0.05\n",
      "\n",
      "---------------------------------------------------------\n",
      "\n",
      "Target: Segment\n",
      "Column: Product\n",
      "\n",
      "chi2: 11.645756859692945\n",
      "Critical Value: 31.410432844230918\n",
      "p: 0.9277293332910025\n",
      "\n",
      "Conclusion:\n",
      "No Correlation\n",
      "\n",
      "p value is above the significance threshold of 0.05\n",
      "\n",
      "---------------------------------------------------------\n",
      "\n",
      "Target: Segment\n",
      "Column: Discount Band\n",
      "\n",
      "chi2: 19.200076992281698\n",
      "Critical Value: 15.50731305586545\n",
      "p: 0.013825488896096009\n",
      "\n",
      "Conclusion:\n",
      "Correlation\n",
      "Cramers V: 0.1218103937222306\n",
      "\n",
      "p value is below the significance threshold of 0.05\n",
      "chi2 value is greater than critical value\n",
      "the difference between observed and expected frequencies is too large to be attributed to chance\n",
      "\n",
      "---------------------------------------------------------\n",
      "\n",
      "Target: Segment\n",
      "Column: Month Name\n",
      "\n",
      "chi2: 0.0\n",
      "Critical Value: 60.480886582336446\n",
      "p: 1.0\n",
      "\n",
      "Conclusion:\n",
      "No Correlation\n",
      "\n",
      "p value is above the significance threshold of 0.05\n",
      "\n",
      "---------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "df = pd.read_excel('C:/Users/Ted/Downloads/FinancialSample.xlsx')\n",
    "#df = pd.read_csv('C:/Users/Ted/Downloads/FinancialSample.csv')\n",
    "\n",
    "target = 'Segment'\n",
    "showCrosstab = False\n",
    "\n",
    "\n",
    "# for every column that is not the target column, check the dtype of that \n",
    "# column and the target column and see if they are both object\n",
    "for column in df.columns:\n",
    "    if target != column:\n",
    "        #object vs object: chi2\n",
    "        # this test compares observed and expected frequencies of the crosstab\n",
    "        if df[target].dtype == 'object' and df[column].dtype == 'object':\n",
    "            print(f'Target: {target}')\n",
    "            print(f'Column: {column}')\n",
    "            print()\n",
    "            ct = pd.crosstab(df[target], df[column])\n",
    "            if showCrosstab == True:\n",
    "                print(ct)\n",
    "                print()\n",
    "            \n",
    "            chi2, p, dof, expected = stats.chi2_contingency(ct)\n",
    "            critical_value = stats.chi2.ppf(1 - 0.05, dof)\n",
    "            \n",
    "            n = np.sum(ct.values) \n",
    "            v = np.sqrt(chi2 / (n * (min(ct.shape) - 1)))\n",
    "            \n",
    "            print(f'chi2: {chi2}')\n",
    "            print(f'Critical Value: {critical_value}')          # this is the value that chi squared must be below or above to determine if it is high or low\n",
    "            print(f'p: {p}')\n",
    "            \n",
    "            print()\n",
    "            print('Conclusion:')\n",
    "            \n",
    "            if p < 0.05:\n",
    "                if chi2 > critical_value:\n",
    "                    print('Correlation')\n",
    "                    print(f'Cramers V: {v}')            # note that Cramers V works but is not suited towards 2x2 contingency tables\n",
    "            \n",
    "                    print()\n",
    "                    print('p value is below the significance threshold of 0.05')\n",
    "                    print('chi2 value is greater than critical value')\n",
    "                    print('the difference between observed and expected frequencies is too large to be attributed to chance')\n",
    "\n",
    "                else: \n",
    "                    print('No Correlation')\n",
    "                    print()\n",
    "                    print('p value is above the significance threshold of 0.05')\n",
    "                    print('chi2 value is less than critical value')\n",
    "                    print('the difference between observed and expected frequencies is too small and could be attributed to chance')\n",
    "                    \n",
    "            else:\n",
    "                print('No Correlation')\n",
    "                print()\n",
    "                print('p value is above the significance threshold of 0.05')\n",
    "            \n",
    "            print()\n",
    "            print('---------------------------------------------------------')\n",
    "            print()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19580ac3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: Units Sold\n",
      "Column: Manufacturing Price\n",
      "\n",
      "x: -0.029643972853024458\n",
      "p: 0.4335823652264368\n",
      "\n",
      "No Correlation\n",
      "Reason: p value is above the significance threshold of 0.05\n",
      "\n",
      "---------------------------------------------------------\n",
      "\n",
      "Target: Units Sold\n",
      "Column: Sale Price\n",
      "\n",
      "x: -0.0650658134215006\n",
      "p: 0.0853913408043615\n",
      "\n",
      "No Correlation\n",
      "Reason: p value is above the significance threshold of 0.05\n",
      "\n",
      "---------------------------------------------------------\n",
      "\n",
      "Target: Units Sold\n",
      "Column: Gross Sales\n",
      "\n",
      "x: 0.32722066254045745\n",
      "p: 6.232960229375536e-19\n",
      "\n",
      "Correlation\n",
      "p value is below the significance threshold of 0.05\n",
      "\n",
      "---------------------------------------------------------\n",
      "\n",
      "Target: Units Sold\n",
      "Column: Discounts\n",
      "\n",
      "x: 0.2530479774656188\n",
      "p: 1.0911815841023358e-11\n",
      "\n",
      "Correlation\n",
      "p value is below the significance threshold of 0.05\n",
      "\n",
      "---------------------------------------------------------\n",
      "\n",
      "Target: Units Sold\n",
      "Column: Sales\n",
      "\n",
      "x: 0.3269139725274527\n",
      "p: 6.747469128497393e-19\n",
      "\n",
      "Correlation\n",
      "p value is below the significance threshold of 0.05\n",
      "\n",
      "---------------------------------------------------------\n",
      "\n",
      "Target: Units Sold\n",
      "Column: COGS\n",
      "\n",
      "x: 0.33169439250128174\n",
      "p: 1.9397416204430017e-19\n",
      "\n",
      "Correlation\n",
      "p value is below the significance threshold of 0.05\n",
      "\n",
      "---------------------------------------------------------\n",
      "\n",
      "Target: Units Sold\n",
      "Column: Profit\n",
      "\n",
      "x: 0.2284369068302451\n",
      "p: 9.7107456560683e-10\n",
      "\n",
      "Correlation\n",
      "p value is below the significance threshold of 0.05\n",
      "\n",
      "---------------------------------------------------------\n",
      "\n",
      "Target: Units Sold\n",
      "Column: Month Number\n",
      "\n",
      "x: -0.10360689389535777\n",
      "p: 0.006075681541677351\n",
      "\n",
      "Correlation\n",
      "p value is below the significance threshold of 0.05\n",
      "\n",
      "---------------------------------------------------------\n",
      "\n",
      "Target: Units Sold\n",
      "Column: Year\n",
      "\n",
      "x: 0.06385657739371733\n",
      "p: 0.09137475522464007\n",
      "\n",
      "No Correlation\n",
      "Reason: p value is above the significance threshold of 0.05\n",
      "\n",
      "---------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "df = pd.read_excel('C:/Users/Ted/Downloads/FinancialSample.xlsx')\n",
    "#df = pd.read_csv('C:/Users/Ted/Downloads/FinancialSample.csv')\n",
    "\n",
    "target = 'Units Sold'\n",
    "\n",
    "# for every column that is not the target column, check the dtype of that \n",
    "# column and the target column and see if they are both object\n",
    "for column in df.columns:\n",
    "    if target != column: \n",
    "        #numerical vs numerical: pearsons or spearmans\n",
    "        if (df[target].dtype == 'float64' or df[target].dtype == 'int64') and (df[column].dtype == 'float64' or df[column].dtype == 'int64'):\n",
    "            x, p = stats.pearsonr(df[target], df[column])       # can use either pearsonr or spearmanr depending on how the data is distributed\n",
    "            print(f'Target: {target}')\n",
    "            print(f'Column: {column}')\n",
    "            print()\n",
    "            \n",
    "            print(f'x: {x}')\n",
    "            print(f'p: {p}')\n",
    "            \n",
    "            print()\n",
    "            \n",
    "            if p < 0.05:\n",
    "                print('Correlation')\n",
    "                print('p value is below the significance threshold of 0.05')    \n",
    "            else:\n",
    "                print('No Correlation')\n",
    "                print('Reason: p value is above the significance threshold of 0.05')\n",
    "            \n",
    "            print()\n",
    "            print('---------------------------------------------------------')\n",
    "            print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b235d415",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "df = pd.read_excel('C:/Users/Ted/Downloads/FinancialSample.xlsx')\n",
    "#df = pd.read_csv('C:/Users/Ted/Downloads/FinancialSample.csv')\n",
    "\n",
    "target = 'Units Sold'\n",
    "showGroupMeans = False\n",
    "\n",
    "# for every column that is not the target column, check the dtype of that \n",
    "# column and the target column and see if they are both object\n",
    "for column in df.columns:\n",
    "    if target != column: \n",
    "        #object vs numerical: ANOVA\n",
    "        #There can be no correlation between nominal vs numerical, only something similar \n",
    "        if ((df[target].dtype == 'float64' or df[target].dtype == 'int64') and df[column].dtype == 'object'):    \n",
    "            \n",
    "            print(f'Target: {target}')\n",
    "            print(f'Column: {column}')  \n",
    "            \n",
    "            if df[target].dtype == 'object':\n",
    "                grouped_data = df.groupby(target)[column].apply(list)\n",
    "            else:\n",
    "                grouped_data = df.groupby(column)[target].apply(list)\n",
    "\n",
    "            # Convert the grouped data into a list of arrays suitable for ANOVA\n",
    "            data_arrays = [group for group in grouped_data]\n",
    "\n",
    "            if showGroupMeans == True:\n",
    "                print()\n",
    "                print('Table of Means by Each Group')\n",
    "                print()\n",
    "                group_means = df.groupby(column)[target].mean().reset_index()\n",
    "                print(round(group_means, 2))\n",
    "\n",
    "            #print(*data_arrays)\n",
    "            #print(grouped_data)\n",
    "            #print(grouped_data)\n",
    "                \n",
    "            # Perform ANOVA\n",
    "            f, p = stats.f_oneway(*data_arrays)\n",
    "            n = len(df.index)        # number of rows\n",
    "            # Finding degrees of freedom for both numerator and demoninator, then using those to help find critical value\n",
    "            dfn, dfd = (len(grouped_data) - 1), (n - 2)      # (2-1) because we are only comparing 2 groups, len(df.index) is number of rows\n",
    "            critical_value = stats.f.ppf(1 - 0.05, dfn, dfd)\n",
    "    \n",
    "            \n",
    "            # Calculating the 3 values here that are needed to find the eta^2\n",
    "            # Calculate Total Sum of Squares (TSS)  \n",
    "            # all_data = np.concatenate(grouped_data)\n",
    "            all_data = []\n",
    "            for group in grouped_data:\n",
    "                all_data.extend(group)\n",
    "            all_data = np.array(all_data)\n",
    "            grand_mean = np.mean(all_data)\n",
    "            tss = np.sum((all_data - grand_mean) ** 2)\n",
    "\n",
    "            # Calculate Within-Group Sum of Squares (WSS)\n",
    "            wss = sum(sum((group - np.mean(group)) ** 2) for group in grouped_data)\n",
    "\n",
    "            # Calculate Between-Group Sum of Squares (BSS)\n",
    "            bss = tss - wss\n",
    "\n",
    "            # Calculate Eta-squared (η²)\n",
    "            eta_squared = bss / tss\n",
    "            \n",
    "            \n",
    "            print()\n",
    "            print(\"f:\", f)\n",
    "            print(f\"Critical Value: {critical_value}\", )\n",
    "            print(\"p:\", p)\n",
    "            print()\n",
    "            \n",
    "            \n",
    "            if eta_squared > 0.14:\n",
    "                doc = 'Large'\n",
    "            elif eta_squared > 0.06:\n",
    "                doc = 'Medium'\n",
    "            elif eta_squared > 0.01:\n",
    "                doc = 'Small'\n",
    "            else:\n",
    "                doc = 'None'\n",
    "            \n",
    "            if p < 0.05:\n",
    "                if f > critical_value:\n",
    "                    print('Correlation:', doc)\n",
    "                    print(\"Eta-squared:\", eta_squared)     \n",
    "                    print()\n",
    "                    #print('p value is below the significance threshold of 0.05')\n",
    "                    #print('f value is greater than critical value')\n",
    "                    #print('the difference between observed and expected frequencies is too large to be attributed to chance')\n",
    "\n",
    "                else: \n",
    "                    print('No Correlation')\n",
    "                    print()\n",
    "                    #print('p value is above the significance threshold of 0.05')\n",
    "                    #print('f value is less than critical value')\n",
    "                    #print('the difference between observed and expected frequencies is too small and could be attributed to chance')\n",
    "                    \n",
    "            else:\n",
    "                print('No Correlation')\n",
    "                print()\n",
    "                print('p value is above the significance threshold of 0.05')\n",
    "            \n",
    "            print()\n",
    "            print('---------------------------------------------------------')\n",
    "            print()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43421592",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df[target].dtype == 'object' and (df[column].dtype == 'float64' or df[column].dtype == 'int64'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "32ef6b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: Country\n",
      "Column: Date\n",
      "Anything vs datetime\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "df = pd.read_excel('C:/Users/Ted/Downloads/FinancialSample.xlsx')\n",
    "#df = pd.read_csv('C:/Users/Ted/Downloads/FinancialSample.csv')\n",
    "\n",
    "target = 'Country'\n",
    "\n",
    "# for every column that is not the target column, check the dtype of that \n",
    "# column and the target column and see if they are both object\n",
    "for column in df.columns:\n",
    "    if target != column:       \n",
    "        #anything vs datetime: Time Series Analysis\n",
    "        if (df[target].dtype == 'object' or df[target].dtype == 'float64' or df[target].dtype == 'int64') and df[column].dtype == 'datetime64[ns]':\n",
    "        #or df[target].dtype == 'datetime64[ns]' and (df[column].dtype == 'object' or df[column].dtype == 'float64' or df[column].dtype == 'int64')\n",
    "            \n",
    "            print(f'Target: {target}')\n",
    "            print(f'Column: {column}')  \n",
    "            print('Anything vs datetime')\n",
    "            # need to add analysis, maybe graph, summary statistics, crosstab, etc?\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4c65c5f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "object - Segment\n",
      "object - Country\n",
      "object - Product\n",
      "object - Discount Band\n",
      "float64 - Units Sold\n",
      "int64 - Manufacturing Price\n",
      "int64 - Sale Price\n",
      "float64 - Gross Sales\n",
      "float64 - Discounts\n",
      "float64 - Sales\n",
      "float64 - COGS\n",
      "float64 - Profit\n",
      "datetime64[ns] - Date\n",
      "int64 - Month Number\n",
      "object - Month Name\n",
      "int64 - Year\n"
     ]
    }
   ],
   "source": [
    "# can change this to report words back such as categorical variable, then move this to the beginning as well as\n",
    "# combine it with summary statistics\n",
    "\n",
    "print()\n",
    "for column in df.columns:\n",
    "    print(f'{df[column].dtype} - {column}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0a9311d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ct = pd.crosstab(df['Country'], df['Product'])\n",
    "#print(ct)\n",
    "#print()\n",
    "\n",
    "#stat, p, dof, expected = stats.chi2_contingency(ct)\n",
    "#critical_value = stats.chi2.ppf(1 - 0.05, dof)\n",
    "\n",
    "#print(stat)\n",
    "#print(critical_value)          # this is the value that chi squared must be below or above to determine if it is high or low\n",
    "#print(p)\n",
    "\n",
    "\n",
    "#tvar = df[target]           # target variable\n",
    "\n",
    "\n",
    "#res = stats.spearmanr(x, y)\n",
    "#print(res)\n",
    "\n",
    "\n",
    "\n",
    "#segment_data = df['Segment']\n",
    "#sales_data = df['Sales']\n",
    "#gross_sales_data = df['Gross Sales']\n",
    "\n",
    "\n",
    "#f_value, p_value = stats.f_oneway(segment_data, sales_data, gross_sales_data)\n",
    "#f_value, p_value = stats.f_oneway(df['Segment'], df['Sales'], df['Gross Sales'])\n",
    "#print(\"F-value:\", f_value, \"P-value:\", p_value)\n",
    "\n",
    "\n",
    "#import numpy as np\n",
    "#import scipy.stats\n",
    "\n",
    "#x = np.array([10, 11, 12, 13, 14, 15, 16, 17, 18, 19])\n",
    "#y = np.array([2, 1, 4, 5, 8, 12, 18, 25, 96, 48])\n",
    "\n",
    "#scipy.stats.pearsonr(x, y)    # Pearson's r\n",
    "#scipy.stats.spearmanr(x, y)   # Spearman's rho\n",
    "#scipy.stats.kendalltau(x, y)  # Kendall's tau\n",
    "\n",
    "\n",
    "#out = scipy.stats.spearmanr(x, y)\n",
    "#print(out)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2c73db96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features based on chi-squared test: ['X2', 'X3']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "# Sample DataFrame with categorical features\n",
    "data = {'X1': [1, 2, 3, 4, 5],\n",
    "        'X2': [5, 4, 3, 2, 1],\n",
    "        'X3': [2, 3, 4, 5, 6],\n",
    "        'y': [1, 0, 1, 0, 1]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop('y', axis=1)\n",
    "y = df['y']\n",
    "\n",
    "# Apply SelectKBest with chi-squared test\n",
    "k = 2  # Number of top features to select\n",
    "chi2_selector = SelectKBest(chi2, k=k)\n",
    "X_kbest = chi2_selector.fit_transform(X, y)\n",
    "\n",
    "# Get selected feature names\n",
    "selected_features = [X.columns[i] for i in chi2_selector.get_support(indices=True)]\n",
    "\n",
    "# Results\n",
    "print(\"Selected features based on chi-squared test:\", selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806981af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
