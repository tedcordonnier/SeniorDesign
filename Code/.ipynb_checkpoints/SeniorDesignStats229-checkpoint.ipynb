{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0c14607c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: Spearmans Correlation Coefficient\n",
      "Target: Sweetness (float64)\n",
      "Column: Size (float64)\n",
      "\n",
      "x: -0.310\n",
      "p: 0.000\n",
      "\n",
      "Correlation: Medium Negative\n",
      "\n",
      "---------------------------------------------------------\n",
      "\n",
      "Test: Spearmans Correlation Coefficient\n",
      "Target: Sweetness (float64)\n",
      "Column: Weight (float64)\n",
      "\n",
      "x: -0.120\n",
      "p: 0.000\n",
      "\n",
      "Correlation: Small Negative\n",
      "\n",
      "---------------------------------------------------------\n",
      "\n",
      "Test: Spearmans Correlation Coefficient\n",
      "Target: Sweetness (float64)\n",
      "Column: Juiciness (float64)\n",
      "\n",
      "x: 0.098\n",
      "p: 0.000\n",
      "\n",
      "Correlation: None\n",
      "\n",
      "---------------------------------------------------------\n",
      "\n",
      "Test: Spearmans Correlation Coefficient\n",
      "Target: Sweetness (float64)\n",
      "Column: Ripeness (float64)\n",
      "\n",
      "x: -0.255\n",
      "p: 0.000\n",
      "\n",
      "Correlation: Small Negative\n",
      "\n",
      "---------------------------------------------------------\n",
      "\n",
      "Test: Spearmans Correlation Coefficient\n",
      "Target: Sweetness (float64)\n",
      "Column: Acidity (float64)\n",
      "\n",
      "x: 0.072\n",
      "p: 0.000\n",
      "\n",
      "Correlation: None\n",
      "\n",
      "---------------------------------------------------------\n",
      "\n",
      "ANOVA on Quality\n",
      "Test: ANOVA and Eta-Squared (η²)\n",
      "Target: Sweetness (float64)\n",
      "Column: Quality (object)\n",
      "\n",
      "f: 268.810\n",
      "cv: 3.844\n",
      "p: 0.000\n",
      "Eta-Squared: 0.063\n",
      "\n",
      "Correlation: Medium\n",
      "\n",
      "---------------------------------------------------------\n",
      "\n",
      "Correlation Between Target and Columns\n",
      "\n",
      "Target: Sweetness\n",
      "\n",
      "Medium\n",
      " - Size\n",
      " - Quality\n",
      "Small\n",
      " - Weight\n",
      " - Ripeness\n",
      "None\n",
      " - A_id\n",
      " - Crunchiness\n",
      " - Juiciness\n",
      " - Acidity\n"
     ]
    }
   ],
   "source": [
    "# Program Purpose: Find Correlation between target column and all other columns. Automates statistical \n",
    "# tests of correlation as well as allows any analyst to find association between all types of variables \n",
    "# without extensive statistical experience.\n",
    "\n",
    "# First, the program takes an input of a excel file or a csv file and\n",
    "# then have the user input a target column. This target will then be compared to all of the \n",
    "# other columns through the use of statistical tests. These tests will then determine correlation\n",
    "# between the target and the columns. The program classifies each of these correlations as\n",
    "# either large, medium, small, or none. The program outputs a list of all of these correlations\n",
    "# based on the category that they were assigned. \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#df = pd.read_excel('C:/Users/Ted/Documents/Data/FinancialSample.xlsx')\n",
    "df = pd.read_csv('C:/Users/Ted/Documents/Data/apple_quality.csv')\n",
    "\n",
    "# Segment and Units Sold are good test samples for FinancialSample\n",
    "target = 'Sweetness'        \n",
    "\n",
    "# Checking to ensure that the target that has been entered is a valid column name\n",
    "if target not in df.columns:\n",
    "    raise ValueError(f\"The target column '{target}' does not exist in the dataframe. Please enter a valid column name.\")\n",
    "\n",
    "bins = int(np.sqrt(len(df[target])))         # Default: int(np.sqrt(len(df[target])))\n",
    "max_unique_values_threshold = 50             # Default: 50. This is the number of categories that an object is limited to having, chi2 gets messed up when there are too many\n",
    "showCrosstab = False\n",
    "showBinnedCrosstab = False         # Depending on the dataset, these crosstabs can be very large and hard to understand but can still glean information from it if interested\n",
    "showGroupMeans = False         \n",
    "includeExplanations = False        # Not implemented, explanations have been commented out\n",
    "\n",
    "# Here the settings section ends, and the program begins \n",
    "\n",
    "correlation_categories = {\n",
    "    \"large\": [],\n",
    "    \"medium\": [],\n",
    "    \"small\": [],\n",
    "    \"none\": [],\n",
    "    \"unable\": []\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# for every column that is not the target column, check the dtype of that \n",
    "# column and the target column and see if they are both object\n",
    "for column in df.columns:\n",
    "    if 'Unnamed' not in column and target != column:\n",
    "        \n",
    "        # this is used to determine if the object (categorical) target or column have too many unique values\n",
    "        # if this is the case, the tests will be unaccurate and should not be used, therefore they are put in the unable column\n",
    "        if (df[target].dtype == 'object' and df[target].nunique() > max_unique_values_threshold):\n",
    "            print(f\"Target '{target}' has {df[column].nunique()} unique values, which is more than the threshold of {max_unique_values_threshold}.\")\n",
    "            print(\"Attempts at correlation would yield unusable results\")\n",
    "            print()\n",
    "            #correlation_categories[\"unable\"].append(column)\n",
    "            break \n",
    "        elif (df[column].dtype == 'object' and df[column].nunique() > max_unique_values_threshold):\n",
    "            #print(f\"Column '{column}' has {df[column].nunique()} unique values, which is more than the threshold.\")\n",
    "            correlation_categories[\"unable\"].append(column)\n",
    "            continue\n",
    "            \n",
    "        \n",
    "        \n",
    "        # object vs object: chi2 and Cramer's V (bias corrected)\n",
    "        # this test compares observed and expected frequencies of the crosstab\n",
    "        if df[target].dtype == 'object' and df[column].dtype == 'object': \n",
    "            \n",
    "            ct = pd.crosstab(df[target], df[column])\n",
    "                       \n",
    "            chi2, p, dof, expected = stats.chi2_contingency(ct)\n",
    "            critical_value = stats.chi2.ppf(1 - 0.05, dof)\n",
    "            \n",
    "            r, k = ct.shape                                      # Number of rows and columns\n",
    "            n = np.sum(ct.values) \n",
    "            k_corrected_term = ((k - 1)**2) / (n - 1)            # Bias correction terms\n",
    "            r_corrected_term = ((r - 1)**2) / (n - 1)\n",
    "            k_corrected = k - k_corrected_term                   # Corrected number of rows and columns for bias correction\n",
    "            r_corrected = r - r_corrected_term\n",
    "                    \n",
    "            if p < 0.05:\n",
    "                if chi2 > critical_value:\n",
    "                    \n",
    "                    print('Test: Chi-Square (Χ²) and Cramers V')\n",
    "                    \n",
    "                    print(f'Target: {target} ({df[target].dtype})')\n",
    "                    print(f'Column: {column} ({df[column].dtype})')\n",
    "                    print()\n",
    "                    \n",
    "                    cramers_v = np.sqrt((chi2 / n) / min(k_corrected - 1, r_corrected - 1))\n",
    "    \n",
    "                    if cramers_v > 0.5:\n",
    "                        doc = 'Large'\n",
    "                        correlation_categories[\"large\"].append(column)\n",
    "                    elif cramers_v > 0.3:\n",
    "                        doc = 'Medium'\n",
    "                        correlation_categories[\"medium\"].append(column)\n",
    "                    elif cramers_v > 0.1:\n",
    "                        doc = 'Small'\n",
    "                        correlation_categories[\"small\"].append(column)\n",
    "                    else:\n",
    "                        doc = 'None'\n",
    "                        correlation_categories[\"none\"].append(column)\n",
    "\n",
    "                    print(f'chi2: {chi2:.3f}')\n",
    "                    print(f'cv: {critical_value:.3f}')          # this is the value that chi squared must be below or above to determine if it is high or low\n",
    "                    print(f'p: {p:.3f}')\n",
    "                    print(f'Cramers V: {cramers_v:.3f}')        # note that Cramers V works but is not suited towards 2x2 contingency tables\n",
    "                    \n",
    "                    print()    \n",
    "                    print('Correlation:', doc)\n",
    "                    \n",
    "                    if showCrosstab == True:\n",
    "                        print()\n",
    "                        print('Crosstab:')\n",
    "                        print()\n",
    "                        print(ct)\n",
    "                        print()\n",
    "                    \n",
    "                    \n",
    "                    #print('p value is below the significance threshold of 0.05')\n",
    "                    #print('chi2 value is greater than critical value')\n",
    "                    #print('the difference between observed and expected frequencies is too large to be attributed to chance')\n",
    "\n",
    "                    print()\n",
    "                    print('---------------------------------------------------------')\n",
    "                    print()\n",
    "                    \n",
    "                else: \n",
    "                    correlation_categories[\"none\"].append(column)\n",
    "                    #print('No Correlation')\n",
    "                    #print()\n",
    "                    #print('p value is above the significance threshold of 0.05')\n",
    "                    #print('chi2 value is less than critical value')\n",
    "                    #print('the difference between observed and expected frequencies is too small and could be attributed to chance')\n",
    "                    \n",
    "            else:\n",
    "                correlation_categories[\"none\"].append(column)\n",
    "                #print('No Correlation')\n",
    "                #print()\n",
    "                #print('p value is above the significance threshold of 0.05')\n",
    "       \n",
    "\n",
    "    \n",
    "\n",
    "        #numerical vs numerical: pearsons or spearmans\n",
    "        elif (df[target].dtype == 'float64' or df[target].dtype == 'int64') and (df[column].dtype == 'float64' or df[column].dtype == 'int64'):\n",
    "            \n",
    "            \n",
    "            x, p = stats.spearmanr(df[target], df[column])       # can use either pearsonr or spearmanr depending on how the data is distributed             \n",
    "            \n",
    "            #print(df[target])\n",
    "            #print(df[column])\n",
    "            #print(target, column, x, p)\n",
    "            \n",
    "            if p < 0.05:\n",
    "                \n",
    "                print('Test: Spearmans Correlation Coefficient')\n",
    "                \n",
    "                if x >= 0.5:\n",
    "                    doc = 'Large Positive'\n",
    "                    correlation_categories[\"large\"].append(column)\n",
    "                elif x >= 0.3:\n",
    "                    doc = 'Medium Positive'\n",
    "                    correlation_categories[\"medium\"].append(column)\n",
    "                elif x >= 0.1:\n",
    "                    doc = 'Small Positive'\n",
    "                    correlation_categories[\"small\"].append(column)\n",
    "                elif x < 0.1 and x > -0.1:\n",
    "                    doc = 'None'\n",
    "                    correlation_categories[\"none\"].append(column)\n",
    "                elif x <= -0.1 and x > -0.3:\n",
    "                    doc = 'Small Negative'\n",
    "                    correlation_categories[\"small\"].append(column)\n",
    "                elif x <= -0.3 and x > -0.5:\n",
    "                    doc = 'Medium Negative'\n",
    "                    correlation_categories[\"medium\"].append(column)\n",
    "                elif x <= -0.5 and x >= -1:\n",
    "                    doc = 'Large Negative'\n",
    "                    correlation_categories[\"large\"].append(column)\n",
    "\n",
    "                print(f'Target: {target} ({df[target].dtype})')\n",
    "                print(f'Column: {column} ({df[column].dtype})')\n",
    "                print()\n",
    "\n",
    "                print(f'x: {x:.3f}')\n",
    "                print(f'p: {p:.3f}')\n",
    "\n",
    "                print()\n",
    "                \n",
    "                print('Correlation:', doc)\n",
    "                #print('p value is below the significance threshold of 0.05')    \n",
    "                \n",
    "                print()\n",
    "                print('---------------------------------------------------------')\n",
    "                print()\n",
    "                \n",
    "            else:\n",
    "                correlation_categories[\"none\"].append(column)\n",
    "                #print('No Correlation')\n",
    "                #print('Reason: p value is above the significance threshold of 0.05')\n",
    "              \n",
    "\n",
    "            \n",
    "            \n",
    "        #numerical vs object: ANOVA\n",
    "        #There can be no technical 'correlation' between numerical vs categorical, only the result is something very similar \n",
    "        elif ((df[target].dtype == 'float64' or df[target].dtype == 'int64') and df[column].dtype == 'object'):    \n",
    "            \n",
    "            grouped_data = df.groupby(column)[target].apply(list)\n",
    "\n",
    "            # Check if at least one group has more than one observation\n",
    "            #if any(len(group) >= 1 for group in grouped_data):\n",
    "            #    correlation_categories[\"unable\"].append(column)\n",
    "            #    continue\n",
    "            #    #print(\"ANOVA requires that at least one input has length greater than 1.\")\n",
    "            \n",
    "            print('ANOVA on ' + column)\n",
    "            \n",
    "            # Convert the grouped data into a list of arrays suitable for ANOVA\n",
    "            data_arrays = [group for group in grouped_data]\n",
    "            \n",
    "\n",
    "            #print(*data_arrays)\n",
    "            #print(grouped_data)\n",
    "            #print(grouped_data)\n",
    "                \n",
    "            # Perform ANOVA\n",
    "            f, p = stats.f_oneway(*data_arrays)\n",
    "            n = len(df.index)        # number of rows\n",
    "            # Finding degrees of freedom for both numerator and demoninator, then using those to help find critical value\n",
    "            dfn, dfd = (len(grouped_data) - 1), (n - 2)      # (2-1) because we are only comparing 2 groups, len(df.index) is number of rows\n",
    "            critical_value = stats.f.ppf(1 - 0.05, dfn, dfd)\n",
    "            \n",
    "            \n",
    "            if p < 0.05:\n",
    "                if f > critical_value:\n",
    "                    \n",
    "                    print('Test: ANOVA and Eta-Squared (η²)')\n",
    "                    \n",
    "                    # Calculating the 3 values here that are needed to find the eta^2\n",
    "                    # Calculate Total Sum of Squares (TSS)  \n",
    "                    # all_data = np.concatenate(grouped_data)\n",
    "                    all_data = []\n",
    "                    for group in grouped_data:\n",
    "                        all_data.extend(group)\n",
    "                    all_data = np.array(all_data)\n",
    "                    grand_mean = np.mean(all_data)\n",
    "                    tss = np.sum((all_data - grand_mean) ** 2)\n",
    "\n",
    "                    # Calculate Within-Group Sum of Squares (WSS)\n",
    "                    wss = sum(sum((group - np.mean(group)) ** 2) for group in grouped_data)\n",
    "\n",
    "                    # Calculate Between-Group Sum of Squares (BSS)\n",
    "                    bss = tss - wss\n",
    "\n",
    "                    # Calculate Eta-squared (η²)\n",
    "                    eta_squared = bss / tss\n",
    "                    \n",
    "                    if eta_squared > 0.14:\n",
    "                        doc = 'Large'\n",
    "                        correlation_categories[\"large\"].append(column)\n",
    "                    elif eta_squared > 0.06:\n",
    "                        doc = 'Medium'\n",
    "                        correlation_categories[\"medium\"].append(column)\n",
    "                    elif eta_squared > 0.01:\n",
    "                        doc = 'Small'\n",
    "                        correlation_categories[\"small\"].append(column)\n",
    "                    else:\n",
    "                        doc = 'None'\n",
    "                        correlation_categories[\"none\"].append(column)\n",
    "                        \n",
    "                    print(f'Target: {target} ({df[target].dtype})')\n",
    "                    print(f'Column: {column} ({df[column].dtype})')\n",
    "                    \n",
    "                    if showGroupMeans == True:\n",
    "                        print()\n",
    "                        print('Table of Means by Each Group')\n",
    "                        print()\n",
    "                        group_means = df.groupby(column)[target].mean().reset_index()\n",
    "                        print(round(group_means, 2))\n",
    "                    \n",
    "                    print()\n",
    "                    print(f\"f: {f:.3f}\")\n",
    "                    print(f\"cv: {critical_value:.3f}\", )\n",
    "                    print(f\"p: {p:.3f}\")\n",
    "                    print(f\"Eta-Squared: {eta_squared:.3f}\")     \n",
    "                    print()\n",
    "                    \n",
    "                    print('Correlation:', doc)\n",
    "                    \n",
    "                    print()\n",
    "                    #print('p value is below the significance threshold of 0.05')\n",
    "                    #print('f value is greater than critical value')\n",
    "                    #print('the difference between observed and expected frequencies is too large to be attributed to chance')\n",
    "\n",
    "                    print('---------------------------------------------------------')\n",
    "                    print()    \n",
    "                    \n",
    "                else:\n",
    "                    correlation_categories[\"none\"].append(column)\n",
    "                    #print('No Correlation')\n",
    "                    #print()\n",
    "                    #print('p value is above the significance threshold of 0.05')\n",
    "                    #print('f value is less than critical value')\n",
    "                    #print('the difference between observed and expected frequencies is too small and could be attributed to chance')\n",
    "                    \n",
    "            else:\n",
    "                correlation_categories[\"none\"].append(column)\n",
    "                #print('No Correlation')\n",
    "                #print()\n",
    "                #print('p value is above the significance threshold of 0.05')\n",
    "                \n",
    "                \n",
    "                \n",
    "        # using chi2 test, however I use bins that separate the numerical variable into a calculated number of bins\n",
    "        # finding correlation between dependent categorical variable and independent continuous variable is quite\n",
    "        # hard, and using chi2 with bins has been the most reasonable approach that I have been able to find\n",
    "        elif (df[target].dtype == 'object' and (df[column].dtype == 'float64' or df[column].dtype == 'int64')):\n",
    "            \n",
    "            bin_edges = np.linspace(1, df[column].max(), num=bins+1)\n",
    "            ct = pd.crosstab(df[target], pd.cut(df[column], bins=bin_edges, right=False, include_lowest=True), margins=False)\n",
    "            #print(pd.cut(df[column], bin_edges).unique())\n",
    "            \n",
    "            chi2, p, dof, expected = stats.chi2_contingency(ct)\n",
    "            critical_value = stats.chi2.ppf(1 - 0.05, dof)\n",
    "            \n",
    "            r, k = ct.shape                                      # Number of rows and columns\n",
    "            n = np.sum(ct.values) \n",
    "            k_corrected_term = ((k - 1)**2) / (n - 1)            # Bias correction terms\n",
    "            r_corrected_term = ((r - 1)**2) / (n - 1)\n",
    "            k_corrected = k - k_corrected_term                   # Corrected number of rows and columns for bias correction\n",
    "            r_corrected = r - r_corrected_term\n",
    "                    \n",
    "            if p < 0.05:\n",
    "                if chi2 > critical_value:\n",
    "                    \n",
    "                    print('Test: Binned Chi-Square (Χ²) and Cramers V')\n",
    "                    \n",
    "                    print(f'Target: {target} ({df[target].dtype})')\n",
    "                    print(f'Column: {column} ({df[column].dtype})')\n",
    "                    print()\n",
    "                    \n",
    "                    cramers_v = np.sqrt((chi2 / n) / min(k_corrected - 1, r_corrected - 1))\n",
    "    \n",
    "                    if cramers_v > 0.5:\n",
    "                        doc = 'Large'\n",
    "                        correlation_categories[\"large\"].append(column)\n",
    "                    elif cramers_v > 0.3:\n",
    "                        doc = 'Medium'\n",
    "                        correlation_categories[\"medium\"].append(column)\n",
    "                    elif cramers_v > 0.1:\n",
    "                        doc = 'Small'\n",
    "                        correlation_categories[\"small\"].append(column)\n",
    "                    else:\n",
    "                        doc = 'None'\n",
    "                        correlation_categories[\"none\"].append(column)\n",
    "\n",
    "                    print(f'chi2: {chi2:.3f}')\n",
    "                    print(f'cv: {critical_value:.3f}')          # this is the value that chi squared must be below or above to determine if it is high or low\n",
    "                    print(f'p: {p:.3f}')\n",
    "                    print(f'Cramers V: {cramers_v:.3f}')        # note that Cramers V works but is not suited towards 2x2 contingency tables\n",
    "                    \n",
    "                    print()    \n",
    "                    print('Correlation:', doc)\n",
    "\n",
    "                    print()\n",
    "                    print('---------------------------------------------------------')\n",
    "                    print()\n",
    "                    \n",
    "                    if showBinnedCrosstab == True:\n",
    "                        print('Binned Crosstab:')\n",
    "                        print()\n",
    "                        print(ct)\n",
    "                        print()\n",
    "                    \n",
    "                else: \n",
    "                    correlation_categories[\"none\"].append(column)\n",
    "\n",
    "            else:\n",
    "                correlation_categories[\"none\"].append(column)\n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "print('Correlation Between Target and Columns')\n",
    "print()\n",
    "print('Target:', target)\n",
    "print()\n",
    "    \n",
    "# printing out the values in each category\n",
    "for category, values in correlation_categories.items():\n",
    "    if values:\n",
    "        print(category.capitalize())\n",
    "        for value in values:\n",
    "            print(f\" - {value}\")\n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12e456fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "float64 - A_id\n",
      "float64 - Size\n",
      "float64 - Weight\n",
      "float64 - Sweetness\n",
      "float64 - Crunchiness\n",
      "float64 - Juiciness\n",
      "float64 - Ripeness\n",
      "object - Acidity\n",
      "object - Quality\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "for column in df.columns:\n",
    "    print(f'{df[column].dtype} - {column}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
