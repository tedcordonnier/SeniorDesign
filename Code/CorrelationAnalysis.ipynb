{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c14607c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: Binned Chi-Square (Χ²) and Cramers V\n",
      "Target: Quality (object)\n",
      "Column: Sweetness (float64)\n",
      "\n",
      "chi2: 78.444\n",
      "cv: 68.669\n",
      "p: 0.008\n",
      "Cramers V: 0.302\n",
      "\n",
      "Correlation: Medium\n",
      "\n",
      "---------------------------------------------------------\n",
      "\n",
      "Test: Binned Chi-Square (Χ²) and Cramers V\n",
      "Target: Quality (object)\n",
      "Column: Crunchiness (float64)\n",
      "\n",
      "chi2: 102.028\n",
      "cv: 65.171\n",
      "p: 0.000\n",
      "Cramers V: 0.226\n",
      "\n",
      "Correlation: Small\n",
      "\n",
      "---------------------------------------------------------\n",
      "\n",
      "Test: Binned Chi-Square (Χ²) and Cramers V\n",
      "Target: Quality (object)\n",
      "Column: Ripeness (float64)\n",
      "\n",
      "chi2: 112.303\n",
      "cv: 68.669\n",
      "p: 0.000\n",
      "Cramers V: 0.267\n",
      "\n",
      "Correlation: Small\n",
      "\n",
      "---------------------------------------------------------\n",
      "\n",
      "Correlation Between Target and Columns\n",
      "\n",
      "Target: Quality\n",
      "\n",
      "Medium\n",
      " - Sweetness\n",
      "Small\n",
      " - Crunchiness\n",
      " - Ripeness\n",
      "None\n",
      " - A_id\n",
      " - Size\n",
      " - Weight\n",
      " - Juiciness\n",
      " - Acidity\n"
     ]
    }
   ],
   "source": [
    "# Program Purpose: Find Correlation between target column and all other columns. Automates statistical \n",
    "# tests of correlation as well as allows any analyst to find association between all types of variables \n",
    "# without extensive statistical experience.\n",
    "\n",
    "# First, the program takes an input of a excel file or a csv file and\n",
    "# then have the user input a target column. This target will then be compared to all of the \n",
    "# other columns through the use of statistical tests. These tests will then determine correlation\n",
    "# between the target and the columns. The program classifies each of these correlations as\n",
    "# either large, medium, small, or none. The program outputs a list of all of these correlations\n",
    "# based on the category that they were assigned. \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# CSV file, path to user input that will be used in correlation analysis\n",
    "df = pd.read_csv('C:/Users/Ted/Documents/Data/apple_quality.csv')                          \n",
    "#df = pd.read_excel('C:/Users/Ted/Documents/Data/FinancialSample.xlsx')\n",
    "target = 'Quality'        \n",
    "\n",
    "# Checking to ensure that the target that has been entered is a valid column name\n",
    "if target not in df.columns:\n",
    "    raise ValueError(f\"The target column '{target}' does not exist in the dataframe. Please enter a valid column name.\")\n",
    "\n",
    "# -------------------------------------------------------------------------------------\n",
    "# Settings\n",
    "    \n",
    "bins = int(np.sqrt(len(df[target])))         # Default: int(np.sqrt(len(df[target])))\n",
    "max_unique_values_threshold = 50             # Default: 50. Only for categorical data. This is the number of categories that an object is limited to having, chi2 gets messed up when there are too many\n",
    "showCrosstab = False                         # Chi2 \n",
    "showBinnedCrosstab = False                   # Binned Chi2, Depending on the dataset, these crosstabs can be very large and hard to understand but can still glean information from it if interested\n",
    "showGroupMeans = False                       # ANOVA    \n",
    "\n",
    "# -------------------------------------------------------------------------------------\n",
    "\n",
    "# Here the settings section ends, and the program begins \n",
    "\n",
    "# Categories for each degree of correlation, each variable will be put into a bucket\n",
    "correlation_categories = {\n",
    "    \"large\": [],\n",
    "    \"medium\": [],\n",
    "    \"small\": [],\n",
    "    \"none\": [],\n",
    "    \"unable\": []\n",
    "}\n",
    "\n",
    "\n",
    "# for every column that is not the target column, check the dtype of that \n",
    "# column and the target column and see if they are both object\n",
    "for column in df.columns:\n",
    "    if 'Unnamed' not in column and target != column:\n",
    "        \n",
    "        # this is used to determine if the object (categorical) target or column have too many unique values\n",
    "        # if this is the case, the tests will be unaccurate and should not be used, therefore they are put in the unable column\n",
    "        if (df[target].dtype == 'object' and df[target].nunique() > max_unique_values_threshold):\n",
    "            print(f\"Target '{target}' has {df[column].nunique()} unique values, which is more than the threshold of {max_unique_values_threshold}.\")\n",
    "            print(\"Attempts at correlation would yield unusable results\")\n",
    "            print()\n",
    "            #correlation_categories[\"unable\"].append(column)\n",
    "            break \n",
    "        elif (df[column].dtype == 'object' and df[column].nunique() > max_unique_values_threshold):\n",
    "            #print(f\"Column '{column}' has {df[column].nunique()} unique values, which is more than the threshold.\")\n",
    "            correlation_categories[\"unable\"].append(column)\n",
    "            continue\n",
    "            \n",
    "        \n",
    "        # object vs object: chi2 and Cramer's V (bias corrected)\n",
    "        # this test compares observed and expected frequencies of the crosstab\n",
    "        if df[target].dtype == 'object' and df[column].dtype == 'object': \n",
    "            \n",
    "            ct = pd.crosstab(df[target], df[column])\n",
    "                       \n",
    "            chi2, p, dof, expected = stats.chi2_contingency(ct)\n",
    "            critical_value = stats.chi2.ppf(1 - 0.05, dof)\n",
    "            \n",
    "            r, k = ct.shape                                      # Number of rows and columns\n",
    "            n = np.sum(ct.values) \n",
    "            k_corrected_term = ((k - 1)**2) / (n - 1)            # Bias correction terms\n",
    "            r_corrected_term = ((r - 1)**2) / (n - 1)\n",
    "            k_corrected = k - k_corrected_term                   # Corrected number of rows and columns for bias correction\n",
    "            r_corrected = r - r_corrected_term\n",
    "                    \n",
    "            if p < 0.05:\n",
    "                if chi2 > critical_value:\n",
    "                    \n",
    "                    print('Test: Chi-Square (Χ²) and Cramers V')\n",
    "                    \n",
    "                    print(f'Target: {target} ({df[target].dtype})')\n",
    "                    print(f'Column: {column} ({df[column].dtype})')\n",
    "                    print()\n",
    "                    \n",
    "                    cramers_v = np.sqrt((chi2 / n) / min(k_corrected - 1, r_corrected - 1))\n",
    "    \n",
    "                    if cramers_v > 0.5:\n",
    "                        doc = 'Large'\n",
    "                        correlation_categories[\"large\"].append(column)\n",
    "                    elif cramers_v > 0.3:\n",
    "                        doc = 'Medium'\n",
    "                        correlation_categories[\"medium\"].append(column)\n",
    "                    elif cramers_v > 0.1:\n",
    "                        doc = 'Small'\n",
    "                        correlation_categories[\"small\"].append(column)\n",
    "                    else:\n",
    "                        doc = 'None'\n",
    "                        correlation_categories[\"none\"].append(column)\n",
    "\n",
    "                    print(f'chi2: {chi2:.3f}')\n",
    "                    print(f'cv: {critical_value:.3f}')          # this is the value that chi squared must be below or above to determine if it is high or low\n",
    "                    print(f'p: {p:.3f}')\n",
    "                    print(f'Cramers V: {cramers_v:.3f}')        # note that Cramers V works but is not suited towards 2x2 contingency tables\n",
    "                    \n",
    "                    print()    \n",
    "                    print('Correlation:', doc)\n",
    "                    \n",
    "                    if showCrosstab == True:\n",
    "                        df_temp = df[[target, column]].copy()\n",
    "                        \n",
    "                        if (df_temp[column].nunique() < 10):\n",
    "         \n",
    "                            # Rename columns for easier readability in crosstab output\n",
    "                            df_temp.rename(columns={target: 'Target', column: 'Column'}, inplace=True)\n",
    "\n",
    "                            # Now use df_temp for crosstab\n",
    "                            ct = pd.crosstab(df_temp['Target'], df_temp['Column'])\n",
    "\n",
    "                            print()\n",
    "                            print('Crosstab:')\n",
    "                            print()\n",
    "                            print(ct)\n",
    "                            print()\n",
    "                        else:\n",
    "                            print()\n",
    "                            print('Too many groups, choosing to not display crosstab')\n",
    "                            \n",
    "                    \n",
    "                    #print('p value is below the significance threshold of 0.05')\n",
    "                    #print('chi2 value is greater than critical value')\n",
    "                    #print('the difference between observed and expected frequencies is too large to be attributed to chance')\n",
    "\n",
    "                    print()\n",
    "                    print('---------------------------------------------------------')\n",
    "                    print()\n",
    "                    \n",
    "                else: \n",
    "                    correlation_categories[\"none\"].append(column)\n",
    "                    #print('No Correlation')\n",
    "                    #print()\n",
    "                    #print('p value is above the significance threshold of 0.05')\n",
    "                    #print('chi2 value is less than critical value')\n",
    "                    #print('the difference between observed and expected frequencies is too small and could be attributed to chance')\n",
    "                    \n",
    "            else:\n",
    "                correlation_categories[\"none\"].append(column)\n",
    "                #print('No Correlation')\n",
    "                #print()\n",
    "                #print('p value is above the significance threshold of 0.05')\n",
    "       \n",
    "\n",
    "    \n",
    "\n",
    "        # numerical vs numerical: pearsons or spearmans\n",
    "        # this test that measures the strength and direction of association between two variables\n",
    "        elif (df[target].dtype == 'float64' or df[target].dtype == 'int64') and (df[column].dtype == 'float64' or df[column].dtype == 'int64'):\n",
    "            \n",
    "            \n",
    "            x, p = stats.spearmanr(df[target], df[column])       # can use either pearsonr or spearmanr depending on how the data is distributed             \n",
    "            \n",
    "            #print(df[target])\n",
    "            #print(df[column])\n",
    "            #print(target, column, x, p)\n",
    "            \n",
    "            if p < 0.05:\n",
    "                \n",
    "                print('Test: Spearmans Correlation Coefficient')\n",
    "                \n",
    "                if x >= 0.5:\n",
    "                    doc = 'Large Positive'\n",
    "                    correlation_categories[\"large\"].append(column)\n",
    "                elif x >= 0.3:\n",
    "                    doc = 'Medium Positive'\n",
    "                    correlation_categories[\"medium\"].append(column)\n",
    "                elif x >= 0.1:\n",
    "                    doc = 'Small Positive'\n",
    "                    correlation_categories[\"small\"].append(column)\n",
    "                elif x < 0.1 and x > -0.1:\n",
    "                    doc = 'None'\n",
    "                    correlation_categories[\"none\"].append(column)\n",
    "                elif x <= -0.1 and x > -0.3:\n",
    "                    doc = 'Small Negative'\n",
    "                    correlation_categories[\"small\"].append(column)\n",
    "                elif x <= -0.3 and x > -0.5:\n",
    "                    doc = 'Medium Negative'\n",
    "                    correlation_categories[\"medium\"].append(column)\n",
    "                elif x <= -0.5 and x >= -1:\n",
    "                    doc = 'Large Negative'\n",
    "                    correlation_categories[\"large\"].append(column)\n",
    "\n",
    "                print(f'Target: {target} ({df[target].dtype})')\n",
    "                print(f'Column: {column} ({df[column].dtype})')\n",
    "                print()\n",
    "\n",
    "                print(f'x: {x:.3f}')\n",
    "                print(f'p: {p:.3f}')\n",
    "\n",
    "                print()\n",
    "                \n",
    "                print('Correlation:', doc)\n",
    "                #print('p value is below the significance threshold of 0.05')    \n",
    "                \n",
    "                print()\n",
    "                print('---------------------------------------------------------')\n",
    "                print()\n",
    "                \n",
    "            else:\n",
    "                correlation_categories[\"none\"].append(column)\n",
    "                #print('No Correlation')\n",
    "                #print('Reason: p value is above the significance threshold of 0.05')\n",
    "              \n",
    "\n",
    "            \n",
    "            \n",
    "        # object vs. numerical: ANOVA\n",
    "        # independent variables: columns. dependent variable: target\n",
    "        # this test uses expected means of each group, and checks to see if there is a \n",
    "        # significant difference between these group means\n",
    "        # There can be no technical 'correlation' between numerical vs categorical, only the result is something very similar \n",
    "        elif ((df[target].dtype == 'float64' or df[target].dtype == 'int64') and df[column].dtype == 'object'):    \n",
    "            \n",
    "            grouped_data = df.groupby(column)[target].apply(list)\n",
    "            #print('ANOVA on ' + column)\n",
    "\n",
    "            # Convert the grouped data into a list of arrays suitable for ANOVA\n",
    "            data_arrays = [group for group in grouped_data]\n",
    "\n",
    "            # Perform ANOVA\n",
    "            if len(data_arrays) > 1:\n",
    "                f, p = stats.f_oneway(*data_arrays)\n",
    "                n = len(df.index)        # number of rows\n",
    "                # Finding degrees of freedom for both numerator and demoninator, then using those to help find critical value\n",
    "                dfn, dfd = (len(grouped_data) - 1), (n - 2)      # (2-1) because we are only comparing 2 groups, len(df.index) is number of rows\n",
    "                critical_value = stats.f.ppf(1 - 0.05, dfn, dfd)\n",
    "            else:\n",
    "                correlation_categories[\"unable\"].append(column)\n",
    "                continue\n",
    "                   \n",
    "            \n",
    "            if p < 0.05:\n",
    "                if f > critical_value:\n",
    "                    \n",
    "                    print('Test: ANOVA and Eta-Squared (η²)')\n",
    "                    \n",
    "                    # Calculating the 3 values here that are needed to find the eta^2\n",
    "                    # Calculate Total Sum of Squares (TSS)  \n",
    "                    # all_data = np.concatenate(grouped_data)\n",
    "                    all_data = []\n",
    "                    for group in grouped_data:\n",
    "                        all_data.extend(group)\n",
    "                    all_data = np.array(all_data)\n",
    "                    grand_mean = np.mean(all_data)\n",
    "                    tss = np.sum((all_data - grand_mean) ** 2)\n",
    "\n",
    "                    # Calculate Within-Group Sum of Squares (WSS)\n",
    "                    wss = sum(sum((group - np.mean(group)) ** 2) for group in grouped_data)\n",
    "\n",
    "                    # Calculate Between-Group Sum of Squares (BSS)\n",
    "                    bss = tss - wss\n",
    "\n",
    "                    # Calculate Eta-squared (η²)\n",
    "                    eta_squared = bss / tss\n",
    "                    \n",
    "                    if eta_squared > 0.14:\n",
    "                        doc = 'Large'\n",
    "                        correlation_categories[\"large\"].append(column)\n",
    "                    elif eta_squared > 0.06:\n",
    "                        doc = 'Medium'\n",
    "                        correlation_categories[\"medium\"].append(column)\n",
    "                    elif eta_squared > 0.01:\n",
    "                        doc = 'Small'\n",
    "                        correlation_categories[\"small\"].append(column)\n",
    "                    else:\n",
    "                        doc = 'None'\n",
    "                        correlation_categories[\"none\"].append(column)\n",
    "                        \n",
    "                    print(f'Target: {target} ({df[target].dtype})')\n",
    "                    print(f'Column: {column} ({df[column].dtype})')\n",
    "                    \n",
    "                    if showGroupMeans == True:\n",
    "                        print()\n",
    "                        print('Table of Means by Each Group')\n",
    "                        print()\n",
    "                        group_means = df.groupby(column)[target].mean().reset_index()\n",
    "                        print(round(group_means, 2))\n",
    "                    \n",
    "                    print()\n",
    "                    print(f\"f: {f:.3f}\")\n",
    "                    print(f\"cv: {critical_value:.3f}\", )\n",
    "                    print(f\"p: {p:.3f}\")\n",
    "                    print(f\"Eta-Squared: {eta_squared:.3f}\")     \n",
    "                    print()\n",
    "                    \n",
    "                    print('Correlation:', doc)\n",
    "                    \n",
    "                    print()\n",
    "                    #print('p value is below the significance threshold of 0.05')\n",
    "                    #print('f value is greater than critical value')\n",
    "                    #print('the difference between observed and expected frequencies is too large to be attributed to chance')\n",
    "\n",
    "                    print('---------------------------------------------------------')\n",
    "                    print()    \n",
    "                    \n",
    "                else:\n",
    "                    correlation_categories[\"none\"].append(column)\n",
    "                    #print('No Correlation')\n",
    "                    #print()\n",
    "                    #print('p value is above the significance threshold of 0.05')\n",
    "                    #print('f value is less than critical value')\n",
    "                    #print('the difference between observed and expected frequencies is too small and could be attributed to chance')\n",
    "                    \n",
    "            else:\n",
    "                correlation_categories[\"none\"].append(column)\n",
    "                #print('No Correlation')\n",
    "                #print()\n",
    "                #print('p value is above the significance threshold of 0.05')\n",
    "                \n",
    "                \n",
    "        # numerical vs. object: Binned chi2 and Cramer's V (bias corrected)\n",
    "        # using chi2 test, however I use bins that separate the numerical variable into a calculated number of bins\n",
    "        # finding correlation between dependent categorical variable and independent continuous variable is quite\n",
    "        # hard, and using chi2 with bins has been the most reasonable approach that I have been able to find\n",
    "        elif (df[target].dtype == 'object' and (df[column].dtype == 'float64' or df[column].dtype == 'int64')):\n",
    "            \n",
    "            bin_edges = np.linspace(1, df[column].max(), num=bins+1)\n",
    "            ct = pd.crosstab(df[target], pd.cut(df[column], bins=bin_edges, right=False, include_lowest=True), margins=False)\n",
    "            #print(pd.cut(df[column], bin_edges).unique())\n",
    "            \n",
    "            chi2, p, dof, expected = stats.chi2_contingency(ct)\n",
    "            critical_value = stats.chi2.ppf(1 - 0.05, dof)\n",
    "            \n",
    "            r, k = ct.shape                                      # Number of rows and columns\n",
    "            n = np.sum(ct.values) \n",
    "            k_corrected_term = ((k - 1)**2) / (n - 1)            # Bias correction terms\n",
    "            r_corrected_term = ((r - 1)**2) / (n - 1)\n",
    "            k_corrected = k - k_corrected_term                   # Corrected number of rows and columns for bias correction\n",
    "            r_corrected = r - r_corrected_term\n",
    "                    \n",
    "            if p < 0.05:\n",
    "                if chi2 > critical_value:\n",
    "                    \n",
    "                    print('Test: Binned Chi-Square (Χ²) and Cramers V')\n",
    "                    \n",
    "                    print(f'Target: {target} ({df[target].dtype})')\n",
    "                    print(f'Column: {column} ({df[column].dtype})')\n",
    "                    print()\n",
    "                    \n",
    "                    cramers_v = np.sqrt((chi2 / n) / min(k_corrected - 1, r_corrected - 1))\n",
    "    \n",
    "                    if cramers_v > 0.5:\n",
    "                        doc = 'Large'\n",
    "                        correlation_categories[\"large\"].append(column)\n",
    "                    elif cramers_v > 0.3:\n",
    "                        doc = 'Medium'\n",
    "                        correlation_categories[\"medium\"].append(column)\n",
    "                    elif cramers_v > 0.1:\n",
    "                        doc = 'Small'\n",
    "                        correlation_categories[\"small\"].append(column)\n",
    "                    else:\n",
    "                        doc = 'None'\n",
    "                        correlation_categories[\"none\"].append(column)\n",
    "\n",
    "                    print(f'chi2: {chi2:.3f}')\n",
    "                    print(f'cv: {critical_value:.3f}')          # this is the value that chi squared must be below or above to determine if it is high or low\n",
    "                    print(f'p: {p:.3f}')\n",
    "                    print(f'Cramers V: {cramers_v:.3f}')        # note that Cramers V works but is not suited towards 2x2 contingency tables\n",
    "                    \n",
    "                    print()    \n",
    "                    print('Correlation:', doc)\n",
    "\n",
    "                    print()\n",
    "                    print('---------------------------------------------------------')\n",
    "                    print()\n",
    "                    \n",
    "                    if showBinnedCrosstab == True:\n",
    "                        print('Binned Crosstab:')\n",
    "                        print()\n",
    "                        print(ct)\n",
    "                        print()\n",
    "                    \n",
    "                else: \n",
    "                    correlation_categories[\"none\"].append(column)\n",
    "\n",
    "            else:\n",
    "                correlation_categories[\"none\"].append(column)\n",
    "            \n",
    "    \n",
    "    \n",
    "# Printing out the final results, to see the strength of correlation of each variable\n",
    "    \n",
    "print('Correlation Between Target and Columns')\n",
    "print()\n",
    "print('Target:', target)\n",
    "print()\n",
    "\n",
    "    \n",
    "# printing out the values in each category\n",
    "for category, values in correlation_categories.items():\n",
    "    if values:\n",
    "        print(category.capitalize())\n",
    "        for value in values:\n",
    "            print(f\" - {value}\")\n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12e456fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "int64 - A_id\n",
      "float64 - Size\n",
      "float64 - Weight\n",
      "float64 - Sweetness\n",
      "float64 - Crunchiness\n",
      "float64 - Juiciness\n",
      "float64 - Ripeness\n",
      "float64 - Acidity\n",
      "object - Quality\n"
     ]
    }
   ],
   "source": [
    "# This allows for the testing of the dataset to determine what datatypes have been automatically detected. \n",
    "# If a column appears as something it is not, please alter the data so that it can be properly processed \n",
    "# as the correct datatype. \n",
    "\n",
    "print()\n",
    "for column in df.columns:\n",
    "    print(f'{df[column].dtype} - {column}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb52553d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
