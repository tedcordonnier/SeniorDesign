{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0c14607c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: Segment\n",
      "Column: Discount Band\n",
      "\n",
      "chi2: 19.200\n",
      "cv: 15.507\n",
      "p: 0.014\n",
      "\n",
      "Correlation: Small\n",
      "Cramers V: 0.122\n",
      "\n",
      "---------------------------------------------------------\n",
      "\n",
      "Target: Segment\n",
      "Column: Sale Price\n",
      "\n",
      "f: 126.895\n",
      "cv: 2.385\n",
      "p: 0.000\n",
      "\n",
      "Eta-squared: 0.422\n",
      "\n",
      "Correlation: Large\n",
      "\n",
      "---------------------------------------------------------\n",
      "\n",
      "Target: Segment\n",
      "Column: Gross Sales\n",
      "\n",
      "f: 69.213\n",
      "cv: 2.385\n",
      "p: 0.000\n",
      "\n",
      "Eta-squared: 0.285\n",
      "\n",
      "Correlation: Large\n",
      "\n",
      "---------------------------------------------------------\n",
      "\n",
      "Target: Segment\n",
      "Column: Discounts\n",
      "\n",
      "f: 44.317\n",
      "cv: 2.385\n",
      "p: 0.000\n",
      "\n",
      "Eta-squared: 0.203\n",
      "\n",
      "Correlation: Large\n",
      "\n",
      "---------------------------------------------------------\n",
      "\n",
      "Target: Segment\n",
      "Column: Sales\n",
      "\n",
      "f: 67.745\n",
      "cv: 2.385\n",
      "p: 0.000\n",
      "\n",
      "Eta-squared: 0.281\n",
      "\n",
      "Correlation: Large\n",
      "\n",
      "---------------------------------------------------------\n",
      "\n",
      "Target: Segment\n",
      "Column: COGS\n",
      "\n",
      "f: 85.769\n",
      "cv: 2.385\n",
      "p: 0.000\n",
      "\n",
      "Eta-squared: 0.330\n",
      "\n",
      "Correlation: Large\n",
      "\n",
      "---------------------------------------------------------\n",
      "\n",
      "Target: Segment\n",
      "Column: Profit\n",
      "\n",
      "f: 36.468\n",
      "cv: 2.385\n",
      "p: 0.000\n",
      "\n",
      "Eta-squared: 0.173\n",
      "\n",
      "Correlation: Large\n",
      "\n",
      "---------------------------------------------------------\n",
      "\n",
      "Correlation Between Target and All Columns\n",
      "\n",
      "Target: Segment\n",
      "\n",
      "Large\n",
      " - Sale Price\n",
      " - Gross Sales\n",
      " - Discounts\n",
      " - Sales\n",
      " - COGS\n",
      " - Profit\n",
      "Small\n",
      " - Discount Band\n",
      "None\n",
      " - Country\n",
      " - Product\n",
      " - Units Sold\n",
      " - Manufacturing Price\n",
      " - Month Number\n",
      " - Month Name\n",
      " - Year\n"
     ]
    }
   ],
   "source": [
    "# Program Purpose: Find Correlation between target column and all other columns. Automates statistical \n",
    "# tests of correlation as well as allows any analyst to find association between all types of variables \n",
    "# without extensive statistical experience.\n",
    "\n",
    "# First, the program takes an input of a excel file or a csv file and\n",
    "# then have the user input a target column. This target will then be compared to all of the \n",
    "# other columns through the use of statistical tests. These tests will then determine correlation\n",
    "# between the target and the columns. The program classifies each of these correlations as\n",
    "# either large, medium, small, or none. The program outputs a list of all of these correlations\n",
    "# based on the category that they were assigned. \n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "df = pd.read_excel('C:/Users/Ted/Downloads/FinancialSample.xlsx')\n",
    "#df = pd.read_csv('C:/Users/Ted/Downloads/FinancialSample.csv')\n",
    "\n",
    "target = 'Segment'         #Segment and Units Sold are good test samples\n",
    "showCrosstab = False\n",
    "showGroupMeans = False        # Not working for now due to the possibility of the order of object and number to be swapped in ANOVA\n",
    "includeExplanations = True    # Not implemented\n",
    "\n",
    "correlation_categories = {\n",
    "    \"large\": [],\n",
    "    \"medium\": [],\n",
    "    \"small\": [],\n",
    "    \"none\": []\n",
    "}\n",
    "\n",
    "# for every column that is not the target column, check the dtype of that \n",
    "# column and the target column and see if they are both object\n",
    "for column in df.columns:\n",
    "    if target != column:\n",
    "        #object vs object: chi2\n",
    "        # this test compares observed and expected frequencies of the crosstab\n",
    "        if df[target].dtype == 'object' and df[column].dtype == 'object':\n",
    "            ct = pd.crosstab(df[target], df[column])\n",
    "            if showCrosstab == True:\n",
    "                print(ct)\n",
    "                print()\n",
    "            \n",
    "            chi2, p, dof, expected = stats.chi2_contingency(ct)\n",
    "            critical_value = stats.chi2.ppf(1 - 0.05, dof)\n",
    "            \n",
    "                    \n",
    "            if p < 0.05:\n",
    "                if chi2 > critical_value:\n",
    "                    \n",
    "                    print(f'Target: {target}')\n",
    "                    print(f'Column: {column}')\n",
    "                    print()\n",
    "                    \n",
    "                    n = np.sum(ct.values) \n",
    "                    cramers_v = np.sqrt(chi2 / (n * (min(ct.shape) - 1)))\n",
    "    \n",
    "                    if cramers_v > 0.5:\n",
    "                        doc = 'Large'\n",
    "                        correlation_categories[\"large\"].append(column)\n",
    "                    elif cramers_v > 0.3:\n",
    "                        doc = 'Medium'\n",
    "                        correlation_categories[\"medium\"].append(column)\n",
    "                    elif cramers_v > 0.1:\n",
    "                        doc = 'Small'\n",
    "                        correlation_categories[\"small\"].append(column)\n",
    "                    else:\n",
    "                        doc = 'None'\n",
    "                        correlation_categories[\"none\"].append(column)\n",
    "\n",
    "                    print(f'chi2: {chi2}')\n",
    "                    print(f'Critical Value: {critical_value}')          # this is the value that chi squared must be below or above to determine if it is high or low\n",
    "                    print(f'p: {p}')\n",
    "                    \n",
    "                    print()    \n",
    "                    print('Correlation:', doc)\n",
    "                    print(f'Cramers V: {cramers_v}')            # note that Cramers V works but is not suited towards 2x2 contingency tables\n",
    "            \n",
    "                    #print('p value is below the significance threshold of 0.05')\n",
    "                    #print('chi2 value is greater than critical value')\n",
    "                    #print('the difference between observed and expected frequencies is too large to be attributed to chance')\n",
    "\n",
    "                    print()\n",
    "                    print('---------------------------------------------------------')\n",
    "                    print()\n",
    "                    \n",
    "                else: \n",
    "                    correlation_categories[\"none\"].append(column)\n",
    "                    #print('No Correlation')\n",
    "                    #print()\n",
    "                    #print('p value is above the significance threshold of 0.05')\n",
    "                    #print('chi2 value is less than critical value')\n",
    "                    #print('the difference between observed and expected frequencies is too small and could be attributed to chance')\n",
    "                    \n",
    "            else:\n",
    "                correlation_categories[\"none\"].append(column)\n",
    "                #print('No Correlation')\n",
    "                #print()\n",
    "                #print('p value is above the significance threshold of 0.05')\n",
    "             \n",
    "            \n",
    "\n",
    "            \n",
    "        #numerical vs numerical: pearsons or spearmans\n",
    "        if (df[target].dtype == 'float64' or df[target].dtype == 'int64') and (df[column].dtype == 'float64' or df[column].dtype == 'int64'):\n",
    "            \n",
    "            x, p = stats.pearsonr(df[target], df[column])       # can use either pearsonr or spearmanr depending on how the data is distributed             \n",
    "            \n",
    "            if p < 0.05:\n",
    "                if x >= 0.5:\n",
    "                    doc = 'Large Positive'\n",
    "                    correlation_categories[\"large\"].append(column)\n",
    "                elif x >= 0.3:\n",
    "                    doc = 'Medium Positive'\n",
    "                    correlation_categories[\"medium\"].append(column)\n",
    "                elif x >= 0.1:\n",
    "                    doc = 'Small Positive'\n",
    "                    correlation_categories[\"small\"].append(column)\n",
    "                elif x < 0.1 and x > -0.1:\n",
    "                    doc = 'None'\n",
    "                    correlation_categories[\"none\"].append(column)\n",
    "                elif x <= -0.1 and x > -0.3:\n",
    "                    doc = 'Small Negative'\n",
    "                    correlation_categories[\"small\"].append(column)\n",
    "                elif x <= -0.3 and x > -0.5:\n",
    "                    doc = 'Medium Negative'\n",
    "                    correlation_categories[\"medium\"].append(column)\n",
    "                elif x <= -0.5 and x >= -1:\n",
    "                    doc = 'Large Negative'\n",
    "                    correlation_categories[\"large\"].append(column)\n",
    "\n",
    "                print(f'Target: {target}')\n",
    "                print(f'Column: {column}')\n",
    "                print()\n",
    "\n",
    "                print(f'x: {x}')\n",
    "                print(f'p: {p}')\n",
    "\n",
    "                print()\n",
    "                \n",
    "                print('Correlation:', doc)\n",
    "                #print('p value is below the significance threshold of 0.05')    \n",
    "                \n",
    "                print()\n",
    "                print('---------------------------------------------------------')\n",
    "                print()\n",
    "                \n",
    "            else:\n",
    "                correlation_categories[\"none\"].append(column)\n",
    "                #print('No Correlation')\n",
    "                #print('Reason: p value is above the significance threshold of 0.05')\n",
    "            \n",
    "            \n",
    "            \n",
    "        #object vs numerical: ANOVA\n",
    "        #There can be no correlation between nominal vs numerical, only something similar \n",
    "        if (df[target].dtype == 'object' and (df[column].dtype == 'float64' or df[column].dtype == 'int64')) or ((df[target].dtype == 'float64' or df[target].dtype == 'int64') and df[column].dtype == 'object'):    \n",
    "            \n",
    "            if df[target].dtype == 'object':\n",
    "                grouped_data = df.groupby(target)[column].apply(list)\n",
    "            else:\n",
    "                grouped_data = df.groupby(column)[target].apply(list)\n",
    "\n",
    "            # Convert the grouped data into a list of arrays suitable for ANOVA\n",
    "            data_arrays = [group for group in grouped_data]\n",
    "\n",
    "            #print(*data_arrays)\n",
    "            #print(grouped_data)\n",
    "            #print(grouped_data)\n",
    "                \n",
    "            # Perform ANOVA\n",
    "            f, p = stats.f_oneway(*data_arrays)\n",
    "            n = len(df.index)        # number of rows\n",
    "            # Finding degrees of freedom for both numerator and demoninator, then using those to help find critical value\n",
    "            dfn, dfd = (len(grouped_data) - 1), (n - 2)      # (2-1) because we are only comparing 2 groups, len(df.index) is number of rows\n",
    "            critical_value = stats.f.ppf(1 - 0.05, dfn, dfd)\n",
    "            \n",
    "            \n",
    "            if p < 0.05:\n",
    "                if f > critical_value:\n",
    "                    \n",
    "                    # Calculating the 3 values here that are needed to find the eta^2\n",
    "                    # Calculate Total Sum of Squares (TSS)  \n",
    "                    # all_data = np.concatenate(grouped_data)\n",
    "                    all_data = []\n",
    "                    for group in grouped_data:\n",
    "                        all_data.extend(group)\n",
    "                    all_data = np.array(all_data)\n",
    "                    grand_mean = np.mean(all_data)\n",
    "                    tss = np.sum((all_data - grand_mean) ** 2)\n",
    "\n",
    "                    # Calculate Within-Group Sum of Squares (WSS)\n",
    "                    wss = sum(sum((group - np.mean(group)) ** 2) for group in grouped_data)\n",
    "\n",
    "                    # Calculate Between-Group Sum of Squares (BSS)\n",
    "                    bss = tss - wss\n",
    "\n",
    "                    # Calculate Eta-squared (η²)\n",
    "                    eta_squared = bss / tss\n",
    "                    \n",
    "                    if eta_squared > 0.14:\n",
    "                        doc = 'Large'\n",
    "                        correlation_categories[\"large\"].append(column)\n",
    "                    elif eta_squared > 0.06:\n",
    "                        doc = 'Medium'\n",
    "                        correlation_categories[\"medium\"].append(column)\n",
    "                    elif eta_squared > 0.01:\n",
    "                        doc = 'Small'\n",
    "                        correlation_categories[\"small\"].append(column)\n",
    "                    else:\n",
    "                        doc = 'None'\n",
    "                        correlation_categories[\"none\"].append(column)\n",
    "                        \n",
    "                    print(f'Target: {target}')\n",
    "                    print(f'Column: {column}')  \n",
    "                    \n",
    "                    if showGroupMeans == True:\n",
    "                        print()\n",
    "                        print('Table of Means by Each Group')\n",
    "                        print()\n",
    "                        group_means = df.groupby(column)[target].mean().reset_index()\n",
    "                        print(round(group_means, 2))\n",
    "                    \n",
    "                    print()\n",
    "                    print(\"f:\", f)\n",
    "                    print(f\"Critical Value: {critical_value}\", )\n",
    "                    print(\"p:\", p)\n",
    "                    print()\n",
    "                    \n",
    "                    print(\"Eta-squared:\", eta_squared)     \n",
    "                    print()\n",
    "                    \n",
    "                    print('Correlation:', doc)\n",
    "                    \n",
    "                    print()\n",
    "                    #print('p value is below the significance threshold of 0.05')\n",
    "                    #print('f value is greater than critical value')\n",
    "                    #print('the difference between observed and expected frequencies is too large to be attributed to chance')\n",
    "\n",
    "                    print('---------------------------------------------------------')\n",
    "                    print()    \n",
    "                    \n",
    "                else:\n",
    "                    correlation_categories[\"none\"].append(column)\n",
    "                    #print('No Correlation')\n",
    "                    #print()\n",
    "                    #print('p value is above the significance threshold of 0.05')\n",
    "                    #print('f value is less than critical value')\n",
    "                    #print('the difference between observed and expected frequencies is too small and could be attributed to chance')\n",
    "                    \n",
    "            else:\n",
    "                correlation_categories[\"none\"].append(column)\n",
    "                #print('No Correlation')\n",
    "                #print()\n",
    "                #print('p value is above the significance threshold of 0.05')\n",
    "                \n",
    "\n",
    "print('Correlation Between Target and All Columns')\n",
    "print()\n",
    "print('Target:', target)\n",
    "print()\n",
    "    \n",
    "# printing out the values in each category\n",
    "for category, values in correlation_categories.items():\n",
    "    if values:\n",
    "        print(category.capitalize())\n",
    "        for value in values:\n",
    "            print(f\" - {value}\")\n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81ef92c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
